[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Just check the PULSE",
    "section": "",
    "text": "This methods section provides a detailed and formal description of the complete data generation process, the principal mechanistic components of the modeling framework, and the parameterization with explicit physiological interpretation for each parameter. The mathematical description emphasizes a continuous-time representation suitable for event-driven cardiac data (R–R intervals), and describes how latent autonomic processes and observation mechanisms combine to produce observable heartbeat sequences. The presentation is structured into three substantive parts that correspond to the requested focus: first, the formal generative model for data; second, an exposition of the model’s internal mechanisms and their interactions; and third, a comprehensive parameter-by-parameter description linking model quantities to physiological constructs and their operational meaning for inference and biomarker construction.\n\n\nLet \\(\\{t_i\\}_{i=1}^{N}\\) denote the observed heartbeat event times recorded over an observation window \\([0,T]\\). The generative model treats heartbeat occurrences as a conditionally inhomogeneous point process whose conditional intensity \\(\\lambda(t\\mid\\mathcal{H}_t)\\) depends on latent autonomic state processes, exogenous inputs, a slowly varying tonic component, and a short-term history dependence term. The generative model is hierarchical: latent continuous-time stochastic processes generate time-varying modulatory signals that determine the conditional hazard; the hazard governs the random occurrence of events; observed R–R intervals are the inter-event times \\(\\Delta t_i = t_i - t_{i-1}\\) produced by the point process; a separate censoring and artifact process may remove or distort certain events to reflect measurement realities.\nThe conditional intensity is specified on the log scale to ensure positivity and to enable additive decomposition of effects. Formally,\n\\[\n\\log\\lambda(t\\mid\\mathcal{H}_t) = \\mu + \\alpha_{p}\\,p(t) - \\alpha_{s}\\,s(t) + \\gamma\\,T_{\\mathrm{circ}}(t) + \\beta\\,u(t) + H(\\mathcal{H}_t),\n\\]\nwhere \\(p(t)\\) and \\(s(t)\\) are latent processes representing parasympathetic and sympathetic activity respectively, \\(T_{\\mathrm{circ}}(t)\\) is a slow tonic or circadian component, \\(u(t)\\) is a known deterministic exogenous input (for example an indicator of exercise), and \\(H(\\mathcal{H}_t)\\) denotes a causal history-dependent term. The signs in the expression are chosen to reflect the canonical physiological directions: elevations in \\(p(t)\\) lengthen interbeat intervals (reduce instantaneous heart rate) so that \\(\\alpha_{p}\\) acts positively on log intensity in conjunction with a negative sign on the time axis mapping; elevations in \\(s(t)\\) shorten interbeat intervals (increase instantaneous heart rate), captured through the negative sign before \\(\\alpha_{s}\\). The baseline level \\(\\mu\\) provides the log-hazard in the absence of modulatory inputs and history effects.\nThe latent autonomic processes evolve under linear stochastic differential equations (SDEs) with coupling terms that permit directed influences between the branches. The SDE formulation preserves the continuous-time nature of underlying physiology and provides a natural way to represent differences in intrinsic time scales and stochastic fluctuation magnitudes. The SDE system is given by\n\\[\n\\begin{aligned}\ndp(t) &= -a_{p}\\,p(t)\\,dt + b_{ps}\\,s(t)\\,dt + c_{p}\\,u(t)\\,dt + \\sigma_{p}\\,dW_{p}(t), \\\\\nds(t) &= -a_{s}\\,s(t)\\,dt + b_{sp}\\,p(t)\\,dt + c_{s}\\,u(t)\\,dt + \\sigma_{s}\\,dW_{s}(t),\n\\end{aligned}\n\\]\nwith \\(a_{p}&gt;0\\) and \\(a_{s}&gt;0\\) being decay rates, \\(b_{ps}\\) and \\(b_{sp}\\) quantifying cross-branch coupling, \\(c_{p}\\) and \\(c_{s}\\) representing exogenous input gains, and \\(\\sigma_{p}\\,dW_{p}\\), \\(\\sigma_{s}\\,dW_{s}\\) representing independent (or, optionally, correlated) Wiener-driven diffusion terms. Initial conditions \\(p(0)\\) and \\(s(0)\\) are drawn from a prescribed prior distribution, typically Gaussian with mean reflecting baseline autonomic tone and variance reflecting prior uncertainty. The tonic component \\(T_{\\mathrm{circ}}(t)\\) can be represented either deterministically, for example by a sinusoid \\(A_{\\mathrm{circ}}\\sin(2\\pi t/24 + \\phi)\\) when diurnal modulation is relevant, or as a low-frequency stochastic drift term when long recordings are considered without explicit periodic structure.\nThe observation process is linked to the point process likelihood. Given the conditional intensity \\(\\lambda(t)\\), the joint density (or likelihood) for event times under the realization of the intensity over \\([0,T]\\) is\n\\[\n\\mathcal{L}(\\{t_i\\}\\mid \\theta, p(\\cdot), s(\\cdot)) = \\left[\\prod_{i=1}^{N}\\lambda(t_i\\mid\\mathcal{H}_{t_i})\\right]\\exp\\left(-\\int_{0}^{T}\\lambda(u\\mid\\mathcal{H}_u)\\,du\\right),\n\\]\nwhere \\(\\theta\\) denotes the collection of static parameters. In practice, the history term \\(H(\\mathcal{H}_t)\\) depends on recent inter-event times and can be implemented as a short memory kernel, such as an exponential kernel applied to previous events or as an autoregressive function of the most recent interbeat interval; this term captures refractoriness, immediate beat-to-beat dependence (including residual respiratory sinus arrhythmia when explicit respiration measurements are absent), and detector-specific short-term structure.\nReal recordings also contain artifacts and missingness. The model accommodates these realities by treating each observed event as potentially censored or corrupted through an auxiliary Bernoulli process that flags events removed due to ectopy, sensor dropout, or erroneous detections. When an event is identified as missing, the likelihood contribution for that interval is replaced by the corresponding survival term under the conditional intensity. The measurement process may also introduce timestamp jitter; small timestamp perturbation can be modeled as additive measurement noise on event times, though for high-quality electrocardiogram-based beat detection this term may be negligible. Where beat annotation uncertainty is substantial, explicit modeling of timestamp noise or probabilistic beat assignment should be included in the generative description.\nNumerical implementation of the continuous-time generative model requires discretization for simulation and likelihood evaluation. The SDEs are discretized with an Euler–Maruyama or higher order scheme on a grid \\(\\{t_k\\}\\) with step \\(\\Delta t\\) chosen to resolve the fastest physiological time constant. For each grid interval the latent state is propagated and the integral of the intensity is approximated by numerical quadrature, typically by summing the intensity evaluated at grid points multiplied by \\(\\Delta t\\) or by Simpson-type rules when greater accuracy is desired. Event generation under simulation is performed by thinning algorithms that sample candidate event times from an upper bound on the intensity and accept them with probability \\(\\lambda(t)/\\lambda_{\\text{max}}\\) to produce event sequences consistent with the instantaneous hazard. The complete data generation process therefore maps static parameters and stochastic latent trajectories to realizations of heartbeat times, subject to censoring and artifact processes that reflect the measurement environment.\n\n\n\nThe modeling framework comprises several interlocking mechanisms, each corresponding to a distinct physiological or statistical function. These mechanisms include the latent autonomic dynamics, the mapping from latent states to instantaneous hazard, the short-term history module, the tonic/circadian modulation, the exogenous input pathway, the stochastic fluctuation processes, and the observation/censoring process. The model is constructed so that each mechanism can be interpreted in physiological terms while remaining amenable to statistically consistent inference.\nThe latent autonomic dynamics mechanism is central to the model. Parasympathetic and sympathetic influences are represented as continuous-valued state variables \\(p(t)\\) and \\(s(t)\\). The linear drift structure \\(-a_{p}p(t)\\) and \\(-a_{s}s(t)\\) imposes a restoring or decay tendency, such that absent input the state relaxes exponentially toward zero with characteristic time constants \\(\\tau_{p}=1/a_{p}\\) and \\(\\tau_{s}=1/a_{s}\\). Coupling terms \\(b_{ps}\\,s(t)\\) and \\(b_{sp}\\,p(t)\\) introduce directed interactions in which the state of one branch influences the rate of change of the other. These coupling coefficients can represent physiologically plausible mechanisms such as reciprocal modulation where sympathetic activation produces vagal withdrawal with a certain delay and magnitude, or more complex facilitative or gating interactions. The exogenous input gains \\(c_{p}\\) and \\(c_{s}\\) route experimentally controlled perturbations (for example, onset of exercise) into changes in autonomic drive, with magnitudes representing the sensitivity of each branch to the perturbation. The diffusion terms, with scale parameters \\(\\sigma_{p}\\) and \\(\\sigma_{s}\\), provide a mechanism for endogenous fluctuation and unmodeled inputs, ensuring that latent trajectories are not strictly deterministic given initial conditions and inputs.\nThe mapping from latent autonomic state to instantaneous hazard is another core mechanism and is implemented through the log-linear observation link. The coefficients \\(\\alpha_{p}\\) and \\(\\alpha_{s}\\) provide the conversion from modulator units to log-intensity units; they determine how a given change in the latent variable affects the instantaneous propensity for the next heartbeat. This mapping embodies the physiological notion that vagal tone has a rapid and strong influence on beat timing while sympathetic tone modulates more slowly and affects baseline heart rate. Because the intensity is in exponential form, shifts in latent state result in multiplicative changes in instantaneous hazard and hence nonlinear effects on the distribution of interbeat intervals, an aspect that captures the non-Gaussian nature of beat-time variability.\nThe short-term history mechanism captures immediate past-dependent effects such as refractory behavior and high-frequency modulation due to respiration. Implementationally, this is realized by including a causal kernel applied to recent events or by including a parametric function of the last interbeat interval. The history kernel produces transient reductions or increases in hazard following an event; for example, an exponential kernel with negative amplitude enforces a short refractory period, reducing the probability of an immediate subsequent beat. The history term thus prevents the latent autonomic variables from attempting to explain phenomena that have inherently short memory and are better modeled as event-driven refractoriness, while also subsuming residual respiratory sinus arrhythmia when respiration is not separately measured.\nThe tonic or circadian mechanism provides low-frequency modulation of baseline susceptibility to beating and is particularly important for prolonged recordings. When present, a sinusoidal or low-frequency stochastic component modulates the baseline \\(\\mu\\) so that diurnal patterns in heart rate are represented. Parameterizing this slow component with amplitude and phase allows the model to separate day–night baseline shifts from short-term autonomic reactivity, thereby improving identification of the dynamic parameters of interest.\nThe observation and censoring mechanism accounts for how latent processes and the instantaneous hazard produce observed event times, and how measurement imperfections intervene. The point process law provides the probabilistic mapping, and the integral term in the likelihood captures the cumulative hazard over recording windows including periods with missing events. When artifacts are detected, the model treats intervals as censored and the corresponding likelihood terms are handled accordingly, preserving statistical coherence. The inclusion of a censoring mechanism is essential for real-world data where ectopic beats, sensor dropout, or annotation uncertainty are unavoidable.\nThese inner components are not independent; they interact in ways that shape identifiability and the dynamics of inferred trajectories. For instance, the history mechanism and the parasympathetic latent process operate on overlapping fast time scales. If the history term is omitted, the parasympathetic latent variable will attempt to explain refractory and respiratory effects, biasing the estimated vagal dynamics. Conversely, an overly aggressive history kernel can absorb genuine vagal reactivity. The coupling terms influence both the transient response to perturbations and the recovery dynamics; positive feedback can amplify transient responses while negative feedback can produce faster return to baseline. The diffusion terms introduce stochasticity that propagates through the mapping to intensity and affects interbeat time dispersion; larger diffusion impairs the precision with which deterministic parameter values can be recovered. The deterministic tonic term interacts with slow sympathetic fluctuations and can confound estimates of tonic sympathetic tone if not properly specified or allowed to vary flexibly.\nExtensibility of the inner components permits the model to accommodate richer physiological hypotheses. Nonlinear drift functions may be used to capture saturating or rectified autonomic responses, state-dependent diffusion can represent heteroscedastic variability that increases with activity, and switching regimes may represent distinct autonomic operating modes such as sleep versus wake or rest versus heavy exercise. Each extension alters inference characteristics and should be justified by prior physiological knowledge and data richness.\n\n\n\nThe model is parameterized by the collection \\(\\theta\\) comprising decay rates, coupling coefficients, input gains, observation gains, diffusion scales, baseline intensity parameters, history kernel parameters, tonic component parameters, and initial condition parameters. Each parameter is described here in terms of its mathematical role, physiological meaning, typical units or scales, typical plausible ranges, identifiability considerations, and practical reparameterizations used to aid inference.\nThe parasympathetic decay rate \\(a_{p}&gt;0\\) appears in the SDE drift \\(-a_{p}p(t)\\). Mathematically it governs the exponential relaxation of \\(p(t)\\) in the absence of inputs. Physiologically it represents the inverse of the characteristic vagal time constant, with \\(\\tau_{p}=1/a_{p}\\) corresponding to the typical duration over which vagally mediated adjustments to heart period decay. Typical values for \\(\\tau_{p}\\) are on the order of one to a few seconds in healthy adult humans under resting conditions; consequently \\(a_{p}\\) is typically in the range of \\(0.2\\) to \\(1.0\\) s\\(^{-1}\\), though exact values depend on scaling conventions for \\(p\\). For identifiability it is often advantageous to adopt a prior that concentrates mass on short time constants while allowing variability across subjects. Reparameterization in terms of \\(\\tau_{p}\\) simplifies prior elicitation and interpretation.\nThe sympathetic decay rate \\(a_{s}&gt;0\\) plays an analogous mathematical role for \\(s(t)\\) and physiologically corresponds to the inverse of the sympathetic time constant \\(\\tau_{s}=1/a_{s}\\). Sympathetic-mediated changes in heart period evolve more slowly than vagal effects; typical \\(\\tau_{s}\\) values are on the order of tens of seconds to several minutes, implying \\(a_{s}\\) values in the approximate range \\(0.005\\) to \\(0.05\\) s\\(^{-1}\\). Because sympathetic time scales can substantially exceed recording segment lengths, identifiability of \\(a_{s}\\) improves with longer recordings or the presence of perturbations that elicit large sympathetic responses.\nThe coupling coefficients \\(b_{ps}\\) and \\(b_{sp}\\) determine directed influences between branches. The parameter \\(b_{ps}\\) multiplies \\(s(t)\\) in the equation for \\(dp(t)\\) and thus quantifies how sympathetic activity drives change in parasympathetic activity; a negative sign would indicate sympathetic-driven vagal withdrawal if the modeled conventions produce such an effect. The parameter \\(b_{sp}\\) quantifies parasympathetic influence on sympathetic dynamics. Physiologically, these coefficients capture interactions mediated by central autonomic networks and reflex arcs, and they provide a way to infer directionality and relative dominance between branches. The natural units are inverse time (s\\(^{-1}\\)); magnitudes are typically small relative to self-decay rates, and their sign and scale require careful prior specification. Identifiability of coupling terms benefits from experimental perturbations and from asymmetries in time constants; in the absence of informative inputs or prolonged recordings, coupling estimates can be poorly constrained and highly correlated with other parameters.\nThe exogenous input gains \\(c_{p}\\) and \\(c_{s}\\) convert a known input signal \\(u(t)\\) into instantaneous influence on the latent processes. For an exercise indicator \\(u(t)\\) that takes values 0 (rest) or 1 (exercise), \\(c_{s}\\) represents the increase in sympathetic drive induced by exercise per unit of \\(u\\), while \\(c_{p}\\) represents the corresponding change to parasympathetic drive. Physiologically, exercise typically produces \\(c_{s}&gt;0\\) and \\(c_{p}&lt;0\\) when \\(p\\) is defined in units such that increases reflect vagal activity. These gains determine the amplitude of the autonomic response to controlled provocations and are directly interpretable as reactivity indices in biomarker construction. Units follow the state scaling; typical magnitudes should be set relative to the scaling of \\(p\\) and \\(s\\), and priors should permit both excitatory and inhibitory effects.\nThe diffusion coefficients \\(\\sigma_{p}\\ge 0\\) and \\(\\sigma_{s}\\ge 0\\) scale the Wiener-driven noise terms and therefore determine the magnitude of stochastic fluctuations in autonomic drive. Mathematically these parameters control variance accumulation over time and thus the ensemble dispersion of latent trajectories. Physiologically larger diffusion reflects greater variability due to unmodeled physiological inputs, central noise, or other modulatory systems. Identifiability of diffusion requires sufficient temporal information to separate process noise from observation-driven variability; over short recordings diffusion and observation link gain parameters can be confounded. Where diffusion is inferred, informative priors or structural constraints can improve robustness.\nThe observation gains \\(\\alpha_{p}&gt;0\\) and \\(\\alpha_{s}&gt;0\\) convert latent autonomic states into contributions to the log-intensity of the point process. The product \\(\\alpha_{p}p(t)\\) specifies how vagal variation translates to instantaneous changes in propensity for a heartbeat; equivalently, the mapping determines the sensitivity of interbeat intervals to autonomic fluctuations. Physiologically vagal influences tend to have larger per-unit effects on beat timing compared with sympathetic influences on short time scales; this asymmetry may be encoded via prior expectations on \\(\\alpha_{p}\\) and \\(\\alpha_{s}\\). Estimation of these gains is crucial for mapping latent-unit fluctuations into interpretable heart-rate changes, and rescaling latent state variables to make \\(\\alpha\\)’s near unity can simplify interpretation and prior elicitation.\nThe baseline log-intensity \\(\\mu\\) sets the nominal heart-beat propensity in the absence of modulatory effects and history. It is closely related to the mean heart rate, with the mean interbeat interval approximately given by the inverse of the expected hazard under stationarity. The prior for \\(\\mu\\) can be centered on the empirical log-rate estimated from observed data, and adjustments to \\(\\mu\\) interact with observation gains in determining overall heart-rate level; hence scaling identifiability issues between \\(\\mu\\) and \\(\\alpha\\) parameters must be monitored.\nThe history kernel parameters control the shape and amplitude of the short-term memory term \\(H(\\mathcal{H}_t)\\). A canonical parametrization is \\(H(\\mathcal{H}_t) = \\int_{0}^{\\infty} h(\\tau) dN(t-\\tau)\\) with \\(h(\\tau)=A_{\\mathrm{hist}}\\exp(-\\tau/\\kappa_{\\mathrm{hist}})\\), where \\(A_{\\mathrm{hist}}\\) is the kernel amplitude and \\(\\kappa_{\\mathrm{hist}}\\) the decay time. The amplitude \\(A_{\\mathrm{hist}}\\) typically assumes negative values to represent refractory suppression immediately following an event, and the decay parameter \\(\\kappa_{\\mathrm{hist}}\\) is on the order of a few hundred milliseconds to a few seconds. These parameters are directly interpretable in terms of immediate electrophysiological recovery and short-term modulation such as RSA when respiration is not otherwise modeled. Identifiability of history parameters is generally good when many beats are observed and the event-generating process is active across short time scales.\nThe tonic/circadian parameters include amplitude \\(A_{\\mathrm{circ}}\\), period (often fixed at 24 hours), and phase \\(\\phi\\). The amplitude quantifies the range of diurnal modulation in log-intensity and is useful for separating slowly varying baseline shifts from dynamic autonomic responses. When long-duration recordings are available, accurate estimation of tonic parameters improves separation of slow sympathetic dynamics from diurnal effects.\nInitial state parameters specify the prior mean and covariance for \\(p(0)\\) and \\(s(0)\\). The prior mean may be set to reflect expected baseline autonomic tone; prior covariance encodes uncertainty and aids numerical stability during inference. Informative priors on initial states can aid identifiability in short segments where latent dynamics have insufficient time to fully express.\nFinally, parameters governing the censoring or artifact process (for example the rate of ectopy or probability of misdetection) describe the measurement environment. These parameters influence the likelihood by dictating which intervals are modeled as observed versus censored and should be chosen based on annotation protocols or estimated jointly when annotation uncertainty is present.\nParameter transformations and constraints facilitate numerical inference. Positivity constraints for decay rates and diffusion scales are implemented via log or softplus transforms; time-constant parametrization $ = 1/a $ provides intuitive priors; rescaling of latent states to place observation gains near unity reduces posterior correlation between gains and baseline \\(\\mu\\). Hierarchical parameterizations that share hyperpriors across subjects allow pooling when multiple recordings are analyzed, improving estimation of parameters that are weakly identified within individual short recordings.\nIn summary, the model parameters each have a clear mathematical role within the SDE-based latent dynamics and point-process observation framework and a direct or interpretable physiological counterpart. The decay rates correspond to characteristic autonomic time constants, coupling coefficients express directed inter-branch interaction, input gains quantify responsiveness to perturbations, observation gains convert latent fluctuations to heart-rate effects, diffusion parameters characterize endogenous variability, history parameters capture event-driven short-term structure, tonic parameters represent slow baseline modulation, and censoring parameters model measurement realities. Careful prior elicitation, thoughtful reparameterization, and judicious use of experimental perturbations are essential to render these parameters identifiable and to translate them into reliable biomarkers of resilience and adaptation."
  },
  {
    "objectID": "index.html#model-formulation",
    "href": "index.html#model-formulation",
    "title": "2026-XX SDE R-R",
    "section": "",
    "text": "Let the observed heartbeat times be denoted by ({t_i}_{i=1}^N) on a recording interval ([0,T]). Cardiac events are modeled as realizations of a conditional intensity function ((t _t)) where (_t) represents the history of event times prior to time (t). The instantaneous hazard is expressed on the log scale as a linear combination of latent autonomic processes, a slowly varying tonic component, an externally controlled experimental input, and a short-term history term that captures refractoriness and immediate beat-to-beat dependencies. Specifically, [ (t _t) = + _p,p(t) - s,s(t) + ,T{}(t) + ,u(t) + H(t), ] where (p(t)) and (s(t)) are continuous-time latent processes representing parasympathetic and sympathetic activity respectively, (T{}(t)) denotes a slowly varying circadian/tonic term, (u(t)) is a known external drive corresponding to experimental condition (for example, an indicator of exercise), and (H(_t)) is a short-memory term that depends on recent inter-event intervals. The signs and scaling are chosen so that increases in (p(t)) correspond to reduced heart rate (longer R–R intervals) and increases in (s(t)) correspond to increased heart rate (shorter R–R intervals).\nThe dynamics of the latent autonomic processes are modeled as linear stochastic differential equations (SDEs) with coupling terms that allow for bidirectional interactions and with additive process noise. The choice of linear drift provides interpretability in terms of time constants and coupling strengths while permitting straightforward numerical discretization and inference. The SDE system is written as [\n\\[\\begin{aligned}\ndp(t) &= -a_p\\,p(t)\\,dt + b_{ps}\\,s(t)\\,dt + c_p\\,u(t)\\,dt + \\sigma_p\\,dW_p(t), \\\\\nds(t) &= -a_s\\,s(t)\\,dt + b_{sp}\\,p(t)\\,dt + c_s\\,u(t)\\,dt + \\sigma_s\\,dW_s(t),\n\\end{aligned}\\]\n] where (a_p) and (a_s) are positive decay rates (inverse time constants) for the parasympathetic and sympathetic processes, (b_{ps}) and (b_{sp}) parameterize cross-branch influences, (c_p) and (c_s) are gains for the external input, and (_p) and (_s) are diffusion coefficients that scale independent Wiener processes (W_p) and (W_s). The slow tonic term is represented as a deterministic sinusoid with period of 24 hours for long-duration recordings or, when not applicable, as a low-frequency drift term. The short-term history contribution is specified as a causal kernel applied to recent event times; operationally this is implemented as a parametric exponential kernel or a low-order autoregressive function of the last inter-event interval to capture refractoriness. The complete generative model is therefore a doubly stochastic point process in which the instantaneous hazard depends on continuous-time latent states that evolve under linear SDE dynamics."
  },
  {
    "objectID": "index.html#likelihood-and-inference-targets",
    "href": "index.html#likelihood-and-inference-targets",
    "title": "2026-XX SDE R-R",
    "section": "",
    "text": "Likelihood computation proceeds from the standard point-process form. Given latent trajectories (p()) and (s()) and parameters (), the log-likelihood of observed heartbeat times satisfies [ (; {t_i}) = {i=1}^{N} (t_i {t_i}) - {0}^{T} (t t),dt. ] Because ((t)) depends on the latent SDE trajectories, the joint inference objective involves the posterior distribution over latent paths and static parameters. Estimation targets include the decay rates (a_p) and (a_s), coupling coefficients (b{ps}) and (b{sp}), gain parameters (c_p), (c_s), (_p), (_s), the process variances (_p^2), (_s^2), the baseline log-intensity (), and parameters governing the history kernel and tonic component. Estimation proceeds under both maximum-likelihood and Bayesian formulations. In the maximum-likelihood approach the latent paths are treated as nuisance variables and integrated out approximately using filtering and smoothing methods. In the Bayesian framework, posterior sampling yields full uncertainty quantification for parameters and latent trajectories."
  },
  {
    "objectID": "index.html#numerical-discretization-and-computational-methods",
    "href": "index.html#numerical-discretization-and-computational-methods",
    "title": "2026-XX SDE R-R",
    "section": "",
    "text": "The continuous-time SDEs are discretized for numerical simulation and filtering using an Euler–Maruyama scheme with a sufficiently fine time step to capture the fast vagal time scale. A default discretization of 0.1 seconds was found to balance numerical accuracy and computational cost for typical adult cardiac dynamics; this choice may be refined according to the sampling density and the expected fastest time constant. The discretized latent state at times (t_k) obeys a linear Gaussian update, which allows for Kalman-type prediction under conditionally Gaussian approximations. The integral term in the point-process log-likelihood is evaluated by numerical quadrature on the discretization grid, exploiting the exponential link to compute piecewise integrals efficiently. For inference we employed three complementary computational strategies. The first strategy is an expectation–maximization (EM) algorithm in which the E-step computes smoothed expectations of the latent states conditional on parameters using an extended or unscented Kalman smoother and the M-step maximizes the expected complete-data log-likelihood with respect to parameters. The second strategy is particle Markov chain Monte Carlo, in particular the particle marginal Metropolis–Hastings (PMMH) algorithm, which uses a bootstrap particle filter to obtain unbiased likelihood estimates that feed into a Metropolis–Hastings sampler for static parameters. The third strategy is variational Bayes, in which an approximating family of factorized distributions over latent trajectories and parameters is optimized to minimize the Kullback–Leibler divergence to the true posterior; variational inference is useful for rapid approximate posteriors on longer recordings.\nThe EM procedure is advantageous for rapid prototyping and yields point estimates consistent with maximum-likelihood principles. In the E-step an unscented Kalman smoother is preferred when nonlinearities are modest, because it better captures posterior covariances without requiring adjoint sensitivity implementations. The M-step exploits the linearity of the discretized latent dynamics to produce closed-form updates or low-dimensional numerical optimizations for subsets of parameters. Particle MCMC provides asymptotically exact posterior samples and is used to validate EM estimates on shorter segments and to produce final uncertainty intervals for biomarkers. Variational methods are implemented with Gaussian process–like variational families for latent trajectories that preserve temporal correlation and avoid degenerate pathwise approximations. For particle methods the number of particles is tuned to achieve reasonable variance in the log-likelihood estimator; practical recommendations are to increase particle counts until acceptance rates for PMMH stabilize and effective sample sizes for latent state approximations during filtering are adequate."
  },
  {
    "objectID": "index.html#prior-specification-and-regularization",
    "href": "index.html#prior-specification-and-regularization",
    "title": "2026-XX SDE R-R",
    "section": "",
    "text": "Prior distributions for parameters incorporate physiological knowledge to aid identifiability. Decay rates (a_p) and (a_s) are given priors that favor plausible time constants; for the parasympathetic process priors concentrate probability mass on short time constants corresponding to second-scale vagal action, whereas priors for the sympathetic decay rate concentrate mass on slower time scales on the order of tens of seconds to minutes. Observation gains (_p) and (_s) are assigned weakly informative normal priors on the log scale or half-normal priors to ensure positivity where required. Coupling coefficients and input gains are centered at zero with modest variance reflecting prior uncertainty about the direction and magnitude of cross-talk. Diffusion coefficients are given half-normal priors that constrain extreme stochasticity but permit substantial latent variability when supported by data. The baseline intensity () is assigned a normal prior informed by the empirical mean heart rate. Regularization in the M-step of EM is implemented via ridge penalties equivalent to Gaussian priors, which stabilizes estimation in finite samples and avoids degenerate likelihood surfaces."
  },
  {
    "objectID": "index.html#preprocessing-and-artifact-handling",
    "href": "index.html#preprocessing-and-artifact-handling",
    "title": "2026-XX SDE R-R",
    "section": "",
    "text": "Prior to model fitting, heartbeat time series undergo artifact detection and correction. Implausible intervals are identified by physiological plausibility thresholds and by local outlier detection based on median absolute deviation; corrections are accomplished by interpolation or removal of spurious beats following established guidelines. For recordings with ectopic beats or missing data segments, affected intervals are excluded from likelihood evaluation and treated as censored, with missing segments bridged by forward simulation of latent dynamics during smoothing. When respiratory recordings are available they are incorporated into (u(t)) or added as an additional covariate; when respiratory data are absent the short-term history term is permitted to absorb residual respiratory sinus arrhythmia effects while acknowledging increased posterior uncertainty in parasympathetic estimates."
  },
  {
    "objectID": "index.html#parameter-interpretability-and-identifiability-assessment",
    "href": "index.html#parameter-interpretability-and-identifiability-assessment",
    "title": "2026-XX SDE R-R",
    "section": "",
    "text": "Model parameters have direct physiological interpretations. The inverse of the decay rates yields characteristic time constants for parasympathetic and sympathetic action; coupling parameters quantify directed influences between branches and provide measures of antagonism or facilitation; input gains quantify reactivity to experimental perturbations. Identifiability is assessed through profile likelihoods, posterior correlation matrices, and sensitivity analyses. Profile likelihoods are computed for targeted parameters to detect flat regions in the likelihood surface. Posterior samples from PMMH are inspected for strong correlations between pairs of parameters, for example between observation gains and baseline intensity, which may indicate nonidentifiable scaling. When identifiability issues are detected, stronger priors informed by physiological literature are applied or the model is simplified by constraining particular coupling terms to zero. In addition, synthetic-data experiments are performed in which known parameter values are used to simulate R–R sequences under the experimental protocol and the inference pipeline is applied to recover parameters. These simulation studies quantify bias, variance, and the influence of recording length and perturbation amplitude on identifiability."
  },
  {
    "objectID": "index.html#biomarker-derivation-and-uncertainty-propagation",
    "href": "index.html#biomarker-derivation-and-uncertainty-propagation",
    "title": "2026-XX SDE R-R",
    "section": "",
    "text": "Biomarkers are derived from posterior summaries of latent trajectories and parameter estimates. Baseline tonic levels of sympathetic and parasympathetic activity are computed as posterior means of (s(t)) and (p(t)) during resting epochs, with credible intervals derived from posterior samples or approximate parameter covariance in maximum-likelihood estimation. Reactivity measures consist of peak changes in latent activity during perturbation periods and are summarized by posterior distributions. Recovery characteristics are quantified by time constants inferred from the estimated decay rates and by empirical exponential fits to the post-perturbation return trajectories of the latent states. Coupling measures derive from the estimated (b_{ps}) and (b_{sp}) coefficients and are reported with uncertainty. Measures of latent variability, computed from (_p) and (_s), are interpreted as indices of regulatory stability and are reported alongside uncertainty intervals. All biomarker computations propagate uncertainty by computing quantities for each posterior draw and summarizing resulting distributions; when variational or EM approximations are used, parametric bootstrap or Laplace approximations are employed to obtain approximate confidence intervals."
  },
  {
    "objectID": "index.html#model-validation-and-diagnostic-procedures",
    "href": "index.html#model-validation-and-diagnostic-procedures",
    "title": "2026-XX SDE R-R",
    "section": "",
    "text": "Model fit is evaluated using posterior predictive checks, residual analysis of time-rescaled inter-event intervals, and comparisons of empirical and simulated spectral characteristics. Posterior predictive checks involve simulating heartbeat sequences from posterior draws of parameters and latent trajectories and comparing empirical distributions of interbeat intervals, their autocorrelation functions, and spectral power distributions to observed data. Time-rescaling residuals are computed by transforming event times via the cumulative hazard and testing for exponentiality and uniformity of transformed inter-event values; departures from theoretical distributions indicate model misspecification primarily in the observation model or history term. Predictive cross-validation is performed by withholding recovery segments and testing the model’s ability to forecast return trajectories from rest and exercise epochs. Sensitivity analyses perturb prior hyperparameters and discretization step sizes to ensure robustness of inferred biomarkers."
  },
  {
    "objectID": "index.html#implementation-and-computational-environment",
    "href": "index.html#implementation-and-computational-environment",
    "title": "2026-XX SDE R-R",
    "section": "",
    "text": "All computations are implemented in Python using a combination of numerical and probabilistic libraries. Discretized SDE integration and Kalman-type smoothing are implemented with NumPy and SciPy linear algebra routines. Particle filtering and PMMH are implemented with JAX-enabled kernels to exploit just-in-time compilation and automatic differentiation where appropriate; for models requiring gradient information in variational optimization, automatic differentiation is employed to obtain parameter gradients. The EM implementation relies on unscented Kalman smoother code adapted from established numerical recipes. Model code, inference scripts, and example notebooks are provided in a publicly archived repository to ensure reproducibility. Computational experiments were executed on multicore CPU instances and typical inference for a 12-minute protocol using EM takes on the order of minutes, whereas full PMMH posterior sampling for high-precision intervals requires hours depending on particle counts and chain length."
  },
  {
    "objectID": "index.html#ethical-considerations-and-data-availability",
    "href": "index.html#ethical-considerations-and-data-availability",
    "title": "2026-XX SDE R-R",
    "section": "",
    "text": "All data used to illustrate and validate the modeling approach are de-identified. Simulated data used for methodological validation are generated under the same observation protocol and are made available along with analysis code in the accompanying repository to facilitate independent replication. The modeling framework is intended to produce interpretable biomarkers for resilience and adaptation; practical application in clinical contexts requires further validation on outcome-labeled cohorts, and any deployment should be accompanied by appropriate clinical oversight."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#complete-data-generation-process",
    "href": "index.html#complete-data-generation-process",
    "title": "Methods",
    "section": "",
    "text": "Observed data are sequences of heartbeat event times \\(\\{t_i\\}_{i=1}^{N}\\) within an observation window \\([0,T]\\). The observed events are treated as realizations of a conditional point process whose instantaneous conditional intensity \\(\\lambda(t\\mid\\mathcal H_t)\\) depends on the history \\(\\mathcal H_t = \\{t_j: t_j &lt; t\\}\\), on deterministic exogenous inputs, and on continuous latent processes that represent autonomic neural drives. Two continuous-time latent processes are defined, \\(p(t)\\) representing parasympathetic (vagal) drive and \\(s(t)\\) representing sympathetic drive. Observed event generation is handled natively in continuous time so that irregular inter-beat intervals are incorporated without resampling or interpolation.\nThe latent dynamics are described by coupled linear stochastic differential equations (SDEs) with additive Gaussian noise. The SDEs are written in Itô form as\n\\[\n\\begin{aligned}\ndp(t) &= \\big(-a_p\\,p(t) + b_{ps}\\,s(t) + c_p\\,u(t)\\big)\\,dt + \\sigma_p\\,dW_p(t),\\\\\nds(t) &= \\big(-a_s\\,s(t) + b_{sp}\\,p(t) + c_s\\,u(t)\\big)\\,dt + \\sigma_s\\,dW_s(t),\n\\end{aligned}\n\\]\nwhere \\(a_p&gt;0\\) and \\(a_s&gt;0\\) are decay rates (inverse time constants) governing exponential relaxation of each branch, \\(b_{ps}\\) and \\(b_{sp}\\) are cross-coupling coefficients, \\(c_p\\) and \\(c_s\\) are gains that map a known deterministic input \\(u(t)\\) into branch-specific drift, \\(\\sigma_p\\) and \\(\\sigma_s\\) are diffusion coefficients, and \\(W_p\\) and \\(W_s\\) are independent standard Wiener processes. The input \\(u(t)\\) may be a binary indicator for experimental perturbation (e.g., exercise/rest) or a continuous covariate such as workload or acceleration magnitude; it is assumed known and deterministic for inference. A slow tonic or circadian modulation \\(T_{\\mathrm{circ}}(t)\\) is included in the observation model to capture baseline drift over long-duration recordings; for recordings shorter than a circadian window \\(T_{\\mathrm{circ}}(t)\\) may be omitted or approximated as a constant offset.\nThe heartbeat generator is a doubly stochastic point process in which the logarithm of the instantaneous intensity is a linear function of the latent variables, the tonic component, the input, and a short-memory history term that captures refractoriness and rapid beat-to-beat dependencies. The log-intensity model is\n\\[\n\\log\\lambda(t\\mid\\mathcal H_t) = \\mu + \\alpha_p\\,p(t) - \\alpha_s\\,s(t) + \\gamma\\,T_{\\mathrm{circ}}(t) + \\beta\\,u(t) + H(\\mathcal H_t),\n\\]\nwith baseline log-hazard \\(\\mu\\), observation gains \\(\\alpha_p,\\alpha_s\\) mapping latent units to log-intensity units, circadian amplitude \\(\\gamma\\), direct input effect \\(\\beta\\), and history kernel \\(H(\\mathcal H_t)\\) which is any causal, finite-memory functional of recent event times or inter-beat intervals. Examples of \\(H(\\mathcal H_t)\\) include an exponential kernel acting on recent spike indicators, a parametric autoregression on the preceding inter-beat interval, or a low-dimensional basis expansion over the last \\(K\\) inter-event times. The sign convention adopted places parasympathetic activity in the direction of reduced instantaneous hazard (slowing heart rate) and sympathetic activity in the direction of increased instantaneous hazard (accelerating heart rate).\nGiven a realization of latent paths and history, the likelihood of observed event times follows the usual point-process form:\n\\[\n\\mathcal L(\\theta; \\{t_i\\}) = \\exp\\Big(\\sum_{i=1}^{N}\\log\\lambda(t_i\\mid\\mathcal H_{t_i}) - \\int_{0}^{T}\\lambda(t\\mid\\mathcal H_t)\\,dt\\Big),\n\\]\nwhere \\(\\theta\\) denotes the collection of static parameters appearing in the latent dynamics and observation mapping. The complete data-generation process is therefore a Cox-type point process driven by continuous-time linear SDE latent dynamics with additive noise, augmented by a short-term history mechanism and an explicit external input."
  },
  {
    "objectID": "index.html#main-mechanisms-of-the-model-and-inner-components",
    "href": "index.html#main-mechanisms-of-the-model-and-inner-components",
    "title": "Just check the PULSE",
    "section": "",
    "text": "The modeling framework comprises several interlocking mechanisms, each corresponding to a distinct physiological or statistical function. These mechanisms include the latent autonomic dynamics, the mapping from latent states to instantaneous hazard, the short-term history module, the tonic/circadian modulation, the exogenous input pathway, the stochastic fluctuation processes, and the observation/censoring process. The model is constructed so that each mechanism can be interpreted in physiological terms while remaining amenable to statistically consistent inference.\nThe latent autonomic dynamics mechanism is central to the model. Parasympathetic and sympathetic influences are represented as continuous-valued state variables \\(p(t)\\) and \\(s(t)\\). The linear drift structure \\(-a_{p}p(t)\\) and \\(-a_{s}s(t)\\) imposes a restoring or decay tendency, such that absent input the state relaxes exponentially toward zero with characteristic time constants \\(\\tau_{p}=1/a_{p}\\) and \\(\\tau_{s}=1/a_{s}\\). Coupling terms \\(b_{ps}\\,s(t)\\) and \\(b_{sp}\\,p(t)\\) introduce directed interactions in which the state of one branch influences the rate of change of the other. These coupling coefficients can represent physiologically plausible mechanisms such as reciprocal modulation where sympathetic activation produces vagal withdrawal with a certain delay and magnitude, or more complex facilitative or gating interactions. The exogenous input gains \\(c_{p}\\) and \\(c_{s}\\) route experimentally controlled perturbations (for example, onset of exercise) into changes in autonomic drive, with magnitudes representing the sensitivity of each branch to the perturbation. The diffusion terms, with scale parameters \\(\\sigma_{p}\\) and \\(\\sigma_{s}\\), provide a mechanism for endogenous fluctuation and unmodeled inputs, ensuring that latent trajectories are not strictly deterministic given initial conditions and inputs.\nThe mapping from latent autonomic state to instantaneous hazard is another core mechanism and is implemented through the log-linear observation link. The coefficients \\(\\alpha_{p}\\) and \\(\\alpha_{s}\\) provide the conversion from modulator units to log-intensity units; they determine how a given change in the latent variable affects the instantaneous propensity for the next heartbeat. This mapping embodies the physiological notion that vagal tone has a rapid and strong influence on beat timing while sympathetic tone modulates more slowly and affects baseline heart rate. Because the intensity is in exponential form, shifts in latent state result in multiplicative changes in instantaneous hazard and hence nonlinear effects on the distribution of interbeat intervals, an aspect that captures the non-Gaussian nature of beat-time variability.\nThe short-term history mechanism captures immediate past-dependent effects such as refractory behavior and high-frequency modulation due to respiration. Implementationally, this is realized by including a causal kernel applied to recent events or by including a parametric function of the last interbeat interval. The history kernel produces transient reductions or increases in hazard following an event; for example, an exponential kernel with negative amplitude enforces a short refractory period, reducing the probability of an immediate subsequent beat. The history term thus prevents the latent autonomic variables from attempting to explain phenomena that have inherently short memory and are better modeled as event-driven refractoriness, while also subsuming residual respiratory sinus arrhythmia when respiration is not separately measured.\nThe tonic or circadian mechanism provides low-frequency modulation of baseline susceptibility to beating and is particularly important for prolonged recordings. When present, a sinusoidal or low-frequency stochastic component modulates the baseline \\(\\mu\\) so that diurnal patterns in heart rate are represented. Parameterizing this slow component with amplitude and phase allows the model to separate day–night baseline shifts from short-term autonomic reactivity, thereby improving identification of the dynamic parameters of interest.\nThe observation and censoring mechanism accounts for how latent processes and the instantaneous hazard produce observed event times, and how measurement imperfections intervene. The point process law provides the probabilistic mapping, and the integral term in the likelihood captures the cumulative hazard over recording windows including periods with missing events. When artifacts are detected, the model treats intervals as censored and the corresponding likelihood terms are handled accordingly, preserving statistical coherence. The inclusion of a censoring mechanism is essential for real-world data where ectopic beats, sensor dropout, or annotation uncertainty are unavoidable.\nThese inner components are not independent; they interact in ways that shape identifiability and the dynamics of inferred trajectories. For instance, the history mechanism and the parasympathetic latent process operate on overlapping fast time scales. If the history term is omitted, the parasympathetic latent variable will attempt to explain refractory and respiratory effects, biasing the estimated vagal dynamics. Conversely, an overly aggressive history kernel can absorb genuine vagal reactivity. The coupling terms influence both the transient response to perturbations and the recovery dynamics; positive feedback can amplify transient responses while negative feedback can produce faster return to baseline. The diffusion terms introduce stochasticity that propagates through the mapping to intensity and affects interbeat time dispersion; larger diffusion impairs the precision with which deterministic parameter values can be recovered. The deterministic tonic term interacts with slow sympathetic fluctuations and can confound estimates of tonic sympathetic tone if not properly specified or allowed to vary flexibly.\nExtensibility of the inner components permits the model to accommodate richer physiological hypotheses. Nonlinear drift functions may be used to capture saturating or rectified autonomic responses, state-dependent diffusion can represent heteroscedastic variability that increases with activity, and switching regimes may represent distinct autonomic operating modes such as sleep versus wake or rest versus heavy exercise. Each extension alters inference characteristics and should be justified by prior physiological knowledge and data richness."
  },
  {
    "objectID": "index.html#model-parameterization-and-physiological-link-of-each-model-parameter",
    "href": "index.html#model-parameterization-and-physiological-link-of-each-model-parameter",
    "title": "Methods",
    "section": "",
    "text": "The vector of static parameters \\(\\theta\\) comprises decay rates \\(a_p, a_s\\), coupling coefficients \\(b_{ps}, b_{sp}\\), input gains \\(c_p, c_s\\), diffusion magnitudes \\(\\sigma_p, \\sigma_s\\), observation gains \\(\\alpha_p, \\alpha_s\\), baseline \\(\\mu\\), tonic amplitude \\(\\gamma\\), direct input log-intensity effect \\(\\beta\\), and parameters describing the history kernel \\(H(\\mathcal H_t)\\). Each parameter admits a physiological interpretation that can be stated quantitatively.\nDecay rates \\(a_p\\) and \\(a_s\\) have units of inverse time; their reciprocals \\(\\tau_p = 1/a_p\\) and \\(\\tau_s = 1/a_s\\) are the characteristic time constants of parasympathetic and sympathetic action. Physiologically expected ranges differ: vagal effects operate over seconds (typical \\(\\tau_p\\) of order 1–5 seconds) whereas sympathetic effects evolve over tens of seconds to minutes (typical \\(\\tau_s\\) of order 30–180 seconds). These expectations inform priors and discretization choices. Cross-coupling terms \\(b_{ps}\\) and \\(b_{sp}\\) quantify directed influence and have units consistent with inverse time. A positive \\(b_{ps}\\) indicates that increasing sympathetic drive tends to increase the drift of parasympathetic activity in the sign convention used; if \\(b_{ps}\\) is negative it indicates direct antagonistic suppression. Likewise \\(b_{sp}\\) captures parasympathetic influence on sympathetic dynamics. The relative magnitudes and signs of these coefficients provide mechanistic interpretation of reciprocal regulation or gating.\nInput gains \\(c_p\\) and \\(c_s\\) scale the deterministic effect of known external covariates \\(u(t)\\). Under exercise, physiology suggests \\(c_s&gt;0\\) and \\(c_p&lt;0\\) when the sign convention maps positive \\(p\\) to heart-rate slowing and positive \\(s\\) to heart-rate acceleration; the absolute magnitudes describe the strength of recruitment and withdrawal respectively. Diffusion coefficients \\(\\sigma_p\\) and \\(\\sigma_s\\) represent the intensity of stochastic fluctuations around deterministic drift, and larger values indicate greater intrinsic variability in central autonomic output or unmodeled afferent influences. Observation gains \\(\\alpha_p\\) and \\(\\alpha_s\\) map latent changes to log-hazard changes; they should be interpreted jointly with the latent variable scalings so that an absolute change in \\(p\\) corresponds to an exponentiated multiplicative change in instantaneous hazard by a factor \\(\\exp(\\alpha_p\\Delta p)\\). Baseline log-hazard \\(\\mu\\) represents the instantaneous hazard in the absence of latent modulation, tonic drift, input, and history terms; it is therefore directly related to baseline heart rate. The circadian amplitude \\(\\gamma\\) quantifies the magnitude of slow rhythmic modulation in log-intensity attributable to time-of-day effects. Parameters of the history kernel control refractoriness magnitude and decay rate; physiologically informed parameter ranges enforce short memory (tens to hundreds of milliseconds) for absolute refractory terms and longer but still sub-second scales for RSA-like effects if respiration is not observed.\nAll parameters may be expressed on transformed scales for inference stability. Positive-constrained parameters such as decay rates and diffusion coefficients are handled on a log scale; observation gains may be constrained to positive values or estimated freely with sign conventions made explicit. Priors are weakly informative but physiologically motivated: priors for \\(a_p\\) concentrate mass around values consistent with sub-10-second vagal time constants, priors for \\(a_s\\) concentrate around longer values, and priors for diffusion coefficients avoid extreme stochasticity that would render the latent processes uninformative. These priors assist identifiability and prevent overfitting in finite-sample regimes."
  },
  {
    "objectID": "index.html#main-inference-algorithm-and-specification",
    "href": "index.html#main-inference-algorithm-and-specification",
    "title": "Methods",
    "section": "",
    "text": "Joint inference of static parameters and continuous latent trajectories is carried out using a hybrid computational strategy that combines expectation–maximization with approximate smoothing for rapid point estimation and particle Markov chain Monte Carlo for asymptotically exact posterior quantification. For very long recordings, a structured variational approach is also available to provide scalable approximate posteriors.\nThe expectation–maximization procedure uses an unscented Kalman smoother (UKS) in the E-step to provide smoothed marginal means and covariances for the latent state on a discretized time grid. Continuous-time SDEs are discretized with an Euler–Maruyama step of size \\(\\Delta t\\), where \\(\\Delta t\\) is chosen to be small relative to the fastest physiological time constant; a default value of \\(\\Delta t=0.1\\) s is recommended for typical adult dynamics but should be reduced when \\(\\tau_p\\) is expected to be shorter. On the discretization grid latent transition matrices are linear and innovations are Gaussian, which permits UKS approximation. The point-process observation likelihood is incorporated by approximating the contribution of log-intensity within each small time bin via a local Laplace expansion or moment-matching so that the observation update at each grid point can be represented as a pseudo-Gaussian correction. The E-step computes expectations of sufficient statistics of the complete-data log-likelihood under the approximate posterior; the M-step maximizes expected complete-data log-likelihood with respect to parameters. Closed-form updates exist for parameters that enter in quadratic forms under Gaussian smoothing; where closed forms are not available quasi-Newton numerical optimization is applied. Convergence is monitored by increases in the observed-data log-likelihood and by stabilization of parameter estimates across iterations.\nExpectation–maximization produces reliable point estimates that are often good initializations for sampling-based approaches. Particle marginal Metropolis–Hastings (PMMH) is used to obtain exact posterior samples of static parameters and to draw posterior latent trajectories conditional on sampled parameters. In PMMH a sequential importance sampling with resampling bootstrap particle filter approximates the marginal likelihood for a proposed static parameter vector. The unbiased likelihood estimate is then used in a Metropolis–Hastings acceptance ratio, yielding a Markov chain that targets the joint posterior of parameters and latent trajectories marginally over the random variables generated by the particle filter. Practical implementation requires tuning the number of particles so that the variance of the log-likelihood estimator is controlled; adaptive schemes increase particle numbers during burn-in or scale particle counts with data length to maintain reasonable acceptance rates. Proposal kernels for static parameters are multivariate normal random walks on transformed parameter scales; adaptation uses empirical covariance of past draws to improve mixing.\nVariational inference constructs a structured Gaussian variational family over latent trajectories with a factorized or blockwise factorization between latent paths and static parameters. The evidence lower bound is optimized via stochastic gradient descent with gradients computed by automatic differentiation through the discretized likelihood and by reparameterization for Gaussian variational factors. Structured variational forms retain temporal correlations and permit efficient computation on long recordings; their approximate posterior uncertainties should be validated against PMMH on representative short segments.\nComputation of the integral term \\(\\int_{0}^{T}\\lambda(t)\\,dt\\) is handled by numerical quadrature on the discretization grid. Adaptive refinement in event-dense regions improves accuracy: when events cluster, the quadrature mesh is refined locally to ensure accurate evaluation of the cumulative hazard. Missing or censored intervals are accommodated by omitting corresponding likelihood contributions and propagating latent states across censored windows in the smoother.\nDiagnostic measures include posterior predictive checks in which synthetic beat trains are generated from posterior draws and compared to empirical inter-event interval distributions, autocorrelation functions, and spectral summaries; time-rescaling residuals transformed via the cumulative hazard are tested for exponentiality to assess observation model fit; and posterior correlations and profile likelihoods are inspected for identifiability issues. Synthetic-data experiments in which ground-truth parameters and latent trajectories are known are used to quantify bias, variance, and identifiability under realistic recording lengths and perturbation strengths."
  },
  {
    "objectID": "index.html#discussion-potential-biomarkers-that-could-be-derived-from-the-model-and-edge-use-cases",
    "href": "index.html#discussion-potential-biomarkers-that-could-be-derived-from-the-model-and-edge-use-cases",
    "title": "Methods",
    "section": "",
    "text": "The inferential output of the model yields time-resolved posterior distributions for latent sympathetic and parasympathetic drives and point estimates and uncertainties for static parameters; these quantities can be transformed into interpretable biomarkers. Baseline tonic levels of \\(s(t)\\) and \\(p(t)\\), estimated as posterior means during defined resting windows, quantify chronic sympathovagal balance and provide potential markers of baseline autonomic tone. Reactivity biomarkers are derived from peak latent excursions and from integrated area-under-curve measures during perturbations: the amplitude of sympathetic surge, the magnitude of vagal withdrawal, and the ratio of these effects capture the magnitude of mobilization. Recovery biomarkers are based on estimated decay parameters \\(a_s\\) and \\(a_p\\) and on empirical exponential fits to latent return trajectories; the time constant for sympathetic withdrawal is a particularly informative index of resilience, with faster withdrawal (larger \\(a_s\\)) interpreted as more efficient regulation. Coupling biomarkers based on \\(b_{ps}\\) and \\(b_{sp}\\) quantify directed reciprocity and can indicate pathological uncoupling or excessive antagonism. Variability indices derived from \\(\\sigma_s\\) and \\(\\sigma_p\\) reflect the noisiness of central autonomic outputs and may track regulatory stability or frailty. Composite biomarkers that combine reactivity, recovery, coupling asymmetry, and variability into a normalized resilience score supply inputs for downstream prediction models and risk stratification.\nEdge use cases include short ambulatory perturbation tests where a brief exercise or cognitive stressor is used to elicit autonomic responses: here the model translates a brief protocol into mechanistic parameters and timed biomarkers that are more physiologically specific than classical spectral HRV metrics. Extended monitoring up to 24 hours benefits from the tonic/circadian component, which provides context for transient responses and permits the study of circadian modulation of reactivity. The model can be applied to intervention studies to quantify mechanistic effects of pharmacological agents or behavioral treatments: changes in \\(a_s\\), \\(a_p\\), \\(c_s\\), \\(c_p\\), or in coupling coefficients constitute interpretable mechanistic endpoints. In diagnostic scenarios where sinoatrial dysfunction is suspected the partition between history-driven effects and latent-driven modulation can help differentiate intrinsic pacemaker irregularity from altered central autonomic regulation.\nLimitations must be acknowledged. When only R–R data are available without respiratory measurements, posterior uncertainty in parasympathetic components increases and the history kernel must absorb RSA-like variability, which complicates physiological attribution. Parameter identifiability may remain limited when perturbations are weak or absent; in such cases stronger priors, experimental augmentation (respiration measurement, posture labels), or pharmacological validation improve interpretability. Nonlinear physiological phenomena such as saturating vagal responses or state-dependent coupling may require model extensions beyond linear SDEs. Careful validation using synthetic data and, where available, ground-truth recordings under pharmacological autonomic blockade provide the strongest evidence for physiological fidelity.\nCollectively, the formal specification and the inference strategy described here provide a reproducible pipeline that converts beat-to-beat event data into physiologically meaningful latent descriptors. The model supports principled uncertainty quantification, yields a rich set of candidate biomarkers for resilience and adaptation, and can be adapted to a range of recording durations and experimental protocols. Implementation artifacts, example code, and simulated datasets should accompany any applied study to ensure that the mapping from raw event times to mechanistic indices is transparent, reproducible, and suitable for subsequent outcome modeling and clinical translation."
  },
  {
    "objectID": "index.html#formal-description-of-the-complete-data-generation-process",
    "href": "index.html#formal-description-of-the-complete-data-generation-process",
    "title": "Just check the PULSE",
    "section": "",
    "text": "Let \\(\\{t_i\\}_{i=1}^{N}\\) denote the observed heartbeat event times recorded over an observation window \\([0,T]\\). The generative model treats heartbeat occurrences as a conditionally inhomogeneous point process whose conditional intensity \\(\\lambda(t\\mid\\mathcal{H}_t)\\) depends on latent autonomic state processes, exogenous inputs, a slowly varying tonic component, and a short-term history dependence term. The generative model is hierarchical: latent continuous-time stochastic processes generate time-varying modulatory signals that determine the conditional hazard; the hazard governs the random occurrence of events; observed R–R intervals are the inter-event times \\(\\Delta t_i = t_i - t_{i-1}\\) produced by the point process; a separate censoring and artifact process may remove or distort certain events to reflect measurement realities.\nThe conditional intensity is specified on the log scale to ensure positivity and to enable additive decomposition of effects. Formally,\n\\[\n\\log\\lambda(t\\mid\\mathcal{H}_t) = \\mu + \\alpha_{p}\\,p(t) - \\alpha_{s}\\,s(t) + \\gamma\\,T_{\\mathrm{circ}}(t) + \\beta\\,u(t) + H(\\mathcal{H}_t),\n\\]\nwhere \\(p(t)\\) and \\(s(t)\\) are latent processes representing parasympathetic and sympathetic activity respectively, \\(T_{\\mathrm{circ}}(t)\\) is a slow tonic or circadian component, \\(u(t)\\) is a known deterministic exogenous input (for example an indicator of exercise), and \\(H(\\mathcal{H}_t)\\) denotes a causal history-dependent term. The signs in the expression are chosen to reflect the canonical physiological directions: elevations in \\(p(t)\\) lengthen interbeat intervals (reduce instantaneous heart rate) so that \\(\\alpha_{p}\\) acts positively on log intensity in conjunction with a negative sign on the time axis mapping; elevations in \\(s(t)\\) shorten interbeat intervals (increase instantaneous heart rate), captured through the negative sign before \\(\\alpha_{s}\\). The baseline level \\(\\mu\\) provides the log-hazard in the absence of modulatory inputs and history effects.\nThe latent autonomic processes evolve under linear stochastic differential equations (SDEs) with coupling terms that permit directed influences between the branches. The SDE formulation preserves the continuous-time nature of underlying physiology and provides a natural way to represent differences in intrinsic time scales and stochastic fluctuation magnitudes. The SDE system is given by\n\\[\n\\begin{aligned}\ndp(t) &= -a_{p}\\,p(t)\\,dt + b_{ps}\\,s(t)\\,dt + c_{p}\\,u(t)\\,dt + \\sigma_{p}\\,dW_{p}(t), \\\\\nds(t) &= -a_{s}\\,s(t)\\,dt + b_{sp}\\,p(t)\\,dt + c_{s}\\,u(t)\\,dt + \\sigma_{s}\\,dW_{s}(t),\n\\end{aligned}\n\\]\nwith \\(a_{p}&gt;0\\) and \\(a_{s}&gt;0\\) being decay rates, \\(b_{ps}\\) and \\(b_{sp}\\) quantifying cross-branch coupling, \\(c_{p}\\) and \\(c_{s}\\) representing exogenous input gains, and \\(\\sigma_{p}\\,dW_{p}\\), \\(\\sigma_{s}\\,dW_{s}\\) representing independent (or, optionally, correlated) Wiener-driven diffusion terms. Initial conditions \\(p(0)\\) and \\(s(0)\\) are drawn from a prescribed prior distribution, typically Gaussian with mean reflecting baseline autonomic tone and variance reflecting prior uncertainty. The tonic component \\(T_{\\mathrm{circ}}(t)\\) can be represented either deterministically, for example by a sinusoid \\(A_{\\mathrm{circ}}\\sin(2\\pi t/24 + \\phi)\\) when diurnal modulation is relevant, or as a low-frequency stochastic drift term when long recordings are considered without explicit periodic structure.\nThe observation process is linked to the point process likelihood. Given the conditional intensity \\(\\lambda(t)\\), the joint density (or likelihood) for event times under the realization of the intensity over \\([0,T]\\) is\n\\[\n\\mathcal{L}(\\{t_i\\}\\mid \\theta, p(\\cdot), s(\\cdot)) = \\left[\\prod_{i=1}^{N}\\lambda(t_i\\mid\\mathcal{H}_{t_i})\\right]\\exp\\left(-\\int_{0}^{T}\\lambda(u\\mid\\mathcal{H}_u)\\,du\\right),\n\\]\nwhere \\(\\theta\\) denotes the collection of static parameters. In practice, the history term \\(H(\\mathcal{H}_t)\\) depends on recent inter-event times and can be implemented as a short memory kernel, such as an exponential kernel applied to previous events or as an autoregressive function of the most recent interbeat interval; this term captures refractoriness, immediate beat-to-beat dependence (including residual respiratory sinus arrhythmia when explicit respiration measurements are absent), and detector-specific short-term structure.\nReal recordings also contain artifacts and missingness. The model accommodates these realities by treating each observed event as potentially censored or corrupted through an auxiliary Bernoulli process that flags events removed due to ectopy, sensor dropout, or erroneous detections. When an event is identified as missing, the likelihood contribution for that interval is replaced by the corresponding survival term under the conditional intensity. The measurement process may also introduce timestamp jitter; small timestamp perturbation can be modeled as additive measurement noise on event times, though for high-quality electrocardiogram-based beat detection this term may be negligible. Where beat annotation uncertainty is substantial, explicit modeling of timestamp noise or probabilistic beat assignment should be included in the generative description.\nNumerical implementation of the continuous-time generative model requires discretization for simulation and likelihood evaluation. The SDEs are discretized with an Euler–Maruyama or higher order scheme on a grid \\(\\{t_k\\}\\) with step \\(\\Delta t\\) chosen to resolve the fastest physiological time constant. For each grid interval the latent state is propagated and the integral of the intensity is approximated by numerical quadrature, typically by summing the intensity evaluated at grid points multiplied by \\(\\Delta t\\) or by Simpson-type rules when greater accuracy is desired. Event generation under simulation is performed by thinning algorithms that sample candidate event times from an upper bound on the intensity and accept them with probability \\(\\lambda(t)/\\lambda_{\\text{max}}\\) to produce event sequences consistent with the instantaneous hazard. The complete data generation process therefore maps static parameters and stochastic latent trajectories to realizations of heartbeat times, subject to censoring and artifact processes that reflect the measurement environment."
  },
  {
    "objectID": "index.html#model-parameterization-and-physiological-interpretation-of-each-parameter",
    "href": "index.html#model-parameterization-and-physiological-interpretation-of-each-parameter",
    "title": "Just check the PULSE",
    "section": "",
    "text": "The model is parameterized by the collection \\(\\theta\\) comprising decay rates, coupling coefficients, input gains, observation gains, diffusion scales, baseline intensity parameters, history kernel parameters, tonic component parameters, and initial condition parameters. Each parameter is described here in terms of its mathematical role, physiological meaning, typical units or scales, typical plausible ranges, identifiability considerations, and practical reparameterizations used to aid inference.\nThe parasympathetic decay rate \\(a_{p}&gt;0\\) appears in the SDE drift \\(-a_{p}p(t)\\). Mathematically it governs the exponential relaxation of \\(p(t)\\) in the absence of inputs. Physiologically it represents the inverse of the characteristic vagal time constant, with \\(\\tau_{p}=1/a_{p}\\) corresponding to the typical duration over which vagally mediated adjustments to heart period decay. Typical values for \\(\\tau_{p}\\) are on the order of one to a few seconds in healthy adult humans under resting conditions; consequently \\(a_{p}\\) is typically in the range of \\(0.2\\) to \\(1.0\\) s\\(^{-1}\\), though exact values depend on scaling conventions for \\(p\\). For identifiability it is often advantageous to adopt a prior that concentrates mass on short time constants while allowing variability across subjects. Reparameterization in terms of \\(\\tau_{p}\\) simplifies prior elicitation and interpretation.\nThe sympathetic decay rate \\(a_{s}&gt;0\\) plays an analogous mathematical role for \\(s(t)\\) and physiologically corresponds to the inverse of the sympathetic time constant \\(\\tau_{s}=1/a_{s}\\). Sympathetic-mediated changes in heart period evolve more slowly than vagal effects; typical \\(\\tau_{s}\\) values are on the order of tens of seconds to several minutes, implying \\(a_{s}\\) values in the approximate range \\(0.005\\) to \\(0.05\\) s\\(^{-1}\\). Because sympathetic time scales can substantially exceed recording segment lengths, identifiability of \\(a_{s}\\) improves with longer recordings or the presence of perturbations that elicit large sympathetic responses.\nThe coupling coefficients \\(b_{ps}\\) and \\(b_{sp}\\) determine directed influences between branches. The parameter \\(b_{ps}\\) multiplies \\(s(t)\\) in the equation for \\(dp(t)\\) and thus quantifies how sympathetic activity drives change in parasympathetic activity; a negative sign would indicate sympathetic-driven vagal withdrawal if the modeled conventions produce such an effect. The parameter \\(b_{sp}\\) quantifies parasympathetic influence on sympathetic dynamics. Physiologically, these coefficients capture interactions mediated by central autonomic networks and reflex arcs, and they provide a way to infer directionality and relative dominance between branches. The natural units are inverse time (s\\(^{-1}\\)); magnitudes are typically small relative to self-decay rates, and their sign and scale require careful prior specification. Identifiability of coupling terms benefits from experimental perturbations and from asymmetries in time constants; in the absence of informative inputs or prolonged recordings, coupling estimates can be poorly constrained and highly correlated with other parameters.\nThe exogenous input gains \\(c_{p}\\) and \\(c_{s}\\) convert a known input signal \\(u(t)\\) into instantaneous influence on the latent processes. For an exercise indicator \\(u(t)\\) that takes values 0 (rest) or 1 (exercise), \\(c_{s}\\) represents the increase in sympathetic drive induced by exercise per unit of \\(u\\), while \\(c_{p}\\) represents the corresponding change to parasympathetic drive. Physiologically, exercise typically produces \\(c_{s}&gt;0\\) and \\(c_{p}&lt;0\\) when \\(p\\) is defined in units such that increases reflect vagal activity. These gains determine the amplitude of the autonomic response to controlled provocations and are directly interpretable as reactivity indices in biomarker construction. Units follow the state scaling; typical magnitudes should be set relative to the scaling of \\(p\\) and \\(s\\), and priors should permit both excitatory and inhibitory effects.\nThe diffusion coefficients \\(\\sigma_{p}\\ge 0\\) and \\(\\sigma_{s}\\ge 0\\) scale the Wiener-driven noise terms and therefore determine the magnitude of stochastic fluctuations in autonomic drive. Mathematically these parameters control variance accumulation over time and thus the ensemble dispersion of latent trajectories. Physiologically larger diffusion reflects greater variability due to unmodeled physiological inputs, central noise, or other modulatory systems. Identifiability of diffusion requires sufficient temporal information to separate process noise from observation-driven variability; over short recordings diffusion and observation link gain parameters can be confounded. Where diffusion is inferred, informative priors or structural constraints can improve robustness.\nThe observation gains \\(\\alpha_{p}&gt;0\\) and \\(\\alpha_{s}&gt;0\\) convert latent autonomic states into contributions to the log-intensity of the point process. The product \\(\\alpha_{p}p(t)\\) specifies how vagal variation translates to instantaneous changes in propensity for a heartbeat; equivalently, the mapping determines the sensitivity of interbeat intervals to autonomic fluctuations. Physiologically vagal influences tend to have larger per-unit effects on beat timing compared with sympathetic influences on short time scales; this asymmetry may be encoded via prior expectations on \\(\\alpha_{p}\\) and \\(\\alpha_{s}\\). Estimation of these gains is crucial for mapping latent-unit fluctuations into interpretable heart-rate changes, and rescaling latent state variables to make \\(\\alpha\\)’s near unity can simplify interpretation and prior elicitation.\nThe baseline log-intensity \\(\\mu\\) sets the nominal heart-beat propensity in the absence of modulatory effects and history. It is closely related to the mean heart rate, with the mean interbeat interval approximately given by the inverse of the expected hazard under stationarity. The prior for \\(\\mu\\) can be centered on the empirical log-rate estimated from observed data, and adjustments to \\(\\mu\\) interact with observation gains in determining overall heart-rate level; hence scaling identifiability issues between \\(\\mu\\) and \\(\\alpha\\) parameters must be monitored.\nThe history kernel parameters control the shape and amplitude of the short-term memory term \\(H(\\mathcal{H}_t)\\). A canonical parametrization is \\(H(\\mathcal{H}_t) = \\int_{0}^{\\infty} h(\\tau) dN(t-\\tau)\\) with \\(h(\\tau)=A_{\\mathrm{hist}}\\exp(-\\tau/\\kappa_{\\mathrm{hist}})\\), where \\(A_{\\mathrm{hist}}\\) is the kernel amplitude and \\(\\kappa_{\\mathrm{hist}}\\) the decay time. The amplitude \\(A_{\\mathrm{hist}}\\) typically assumes negative values to represent refractory suppression immediately following an event, and the decay parameter \\(\\kappa_{\\mathrm{hist}}\\) is on the order of a few hundred milliseconds to a few seconds. These parameters are directly interpretable in terms of immediate electrophysiological recovery and short-term modulation such as RSA when respiration is not otherwise modeled. Identifiability of history parameters is generally good when many beats are observed and the event-generating process is active across short time scales.\nThe tonic/circadian parameters include amplitude \\(A_{\\mathrm{circ}}\\), period (often fixed at 24 hours), and phase \\(\\phi\\). The amplitude quantifies the range of diurnal modulation in log-intensity and is useful for separating slowly varying baseline shifts from dynamic autonomic responses. When long-duration recordings are available, accurate estimation of tonic parameters improves separation of slow sympathetic dynamics from diurnal effects.\nInitial state parameters specify the prior mean and covariance for \\(p(0)\\) and \\(s(0)\\). The prior mean may be set to reflect expected baseline autonomic tone; prior covariance encodes uncertainty and aids numerical stability during inference. Informative priors on initial states can aid identifiability in short segments where latent dynamics have insufficient time to fully express.\nFinally, parameters governing the censoring or artifact process (for example the rate of ectopy or probability of misdetection) describe the measurement environment. These parameters influence the likelihood by dictating which intervals are modeled as observed versus censored and should be chosen based on annotation protocols or estimated jointly when annotation uncertainty is present.\nParameter transformations and constraints facilitate numerical inference. Positivity constraints for decay rates and diffusion scales are implemented via log or softplus transforms; time-constant parametrization $ = 1/a $ provides intuitive priors; rescaling of latent states to place observation gains near unity reduces posterior correlation between gains and baseline \\(\\mu\\). Hierarchical parameterizations that share hyperpriors across subjects allow pooling when multiple recordings are analyzed, improving estimation of parameters that are weakly identified within individual short recordings.\nIn summary, the model parameters each have a clear mathematical role within the SDE-based latent dynamics and point-process observation framework and a direct or interpretable physiological counterpart. The decay rates correspond to characteristic autonomic time constants, coupling coefficients express directed inter-branch interaction, input gains quantify responsiveness to perturbations, observation gains convert latent fluctuations to heart-rate effects, diffusion parameters characterize endogenous variability, history parameters capture event-driven short-term structure, tonic parameters represent slow baseline modulation, and censoring parameters model measurement realities. Careful prior elicitation, thoughtful reparameterization, and judicious use of experimental perturbations are essential to render these parameters identifiable and to translate them into reliable biomarkers of resilience and adaptation."
  },
  {
    "objectID": "docs/point-process.html",
    "href": "docs/point-process.html",
    "title": "Simulating Point Processes in R",
    "section": "",
    "text": "A point process is a stochastic collection of events occurring in continuous time. In physiology, the most common realization of a point process is a neural spike train—a sequence of action potentials generated by a neuron. Other examples include heartbeats (R-peaks in ECG), vesicle release events at a synapse, or spontaneous muscle twitches.\nResearchers often model these events using the conditional intensity function (CIF), denoted as \\(\\lambda(t | H_t)\\). This function represents the instantaneous probability rate of an event occurring at time \\(t\\), given the entire history of events \\(H_t\\) up to that moment.\nFor a physiologist, \\(\\lambda(t | H_t)\\) answers the question: Given the stimulus applied right now, and given how recently the neuron fired previously, how likely is it to fire in the next millisecond?\nThis guide details the algorithms required to simulate these processes in R. It progresses from memoryless constant-rate processes to complex, history-dependent systems that mimic bursting and refractory periods.\n\n\nThe conditional intensity function is defined as:\n\\[\n\\lambda(t | H_t) = \\lim_{\\Delta t \\to 0} \\frac{P(\\text{event in } (t, t+\\Delta t] | H_t)}{\\Delta t}\n\\]\nIf \\(\\lambda(t | H_t)\\) is constant, the process is a Homogeneous Poisson Process. If \\(\\lambda(t | H_t)\\) depends on time \\(t\\) but not history \\(H_t\\), it is an Inhomogeneous Poisson Process. If it depends on \\(H_t\\), it is a general point process (e.g., a renewal process or Hawkes process).\n\n\n\nScenario: A neuron firing due to random thermal noise with no external stimulus and no refractory period.\nThis is the simplest baseline. The rate \\(\\lambda\\) is constant. A defining property of the homogeneous Poisson process is that the time intervals between events (Inter-Spike Intervals, or ISIs) follow an Exponential distribution.\nTo simulate this, we do not need complex loops. We generate the intervals directly and take their cumulative sum to find the event times.\n\n\n\n#' Simulate Homogeneous Poisson Process\n#' \n#' @param rate The constant firing rate (Hz)\n#' @param duration Total time to simulate (seconds)\n#' @return Vector of event times\nsim_homogeneous &lt;- function(rate, duration) {\n  # 1. Estimate expected number of events to allocate memory\n  # We add a margin (e.g., 20%) to ensure we cover the duration\n  n_guess &lt;- ceiling(rate * duration * 1.2)\n  \n  # 2. Generate Inter-Spike Intervals (ISIs) from Exponential distribution\n  # The rate parameter for rexp is lambda\n  isis &lt;- rexp(n_guess, rate = rate)\n  \n  # 3. Convert intervals to absolute times\n  event_times &lt;- cumsum(isis)\n  \n  # 4. Filter events that exceed the duration\n  event_times &lt;- event_times[event_times &lt;= duration]\n  \n  return(event_times)\n}\n\n# Example Usage\nset.seed(42)\nlambda_constant &lt;- 10 # 10 Hz\nT_max &lt;- 10           # 10 seconds\n\nspikes_homo &lt;- sim_homogeneous(lambda_constant, T_max)\n\n# Visualization\npar(mfrow = c(1, 2))\n# Raster plot (first 1 second)\nstripchart(spikes_homo[spikes_homo &lt; 1], method = \"jitter\", pch = \"|\", \n           main = \"Raster Plot (First 1s)\", xlab = \"Time (s)\")\n# Histogram of ISIs\nhist(diff(spikes_homo), breaks = 30, main = \"ISI Histogram\", \n     xlab = \"Inter-Spike Interval (s)\", freq = FALSE)\ncurve(dexp(x, rate = lambda_constant), add = TRUE, col = \"red\")\n\n\n\n\n\n\n\n\nThe histogram of ISIs aligns with the theoretical exponential density curve (red line), confirming the memoryless nature of the process.\n\n\n\n\nScenario: A retinal neuron responding to a visual stimulus that changes brightness over time. The firing rate changes, but the neuron still has no “memory” (no refractory period).\nHere, \\(\\lambda(t)\\) varies with time. We can no longer simply draw from an exponential distribution because the rate parameter changes continuously.\n\n\nThe standard method for simulating this is Ogata’s Thinning Algorithm (a variation of Lewis and Shedler’s method). The logic follows a “generate and reject” principle:\n\nDetermine an upper bound \\(\\lambda_{max}\\) such that \\(\\lambda(t) \\le \\lambda_{max}\\) for all \\(t\\).\nGenerate a candidate event time using the constant rate \\(\\lambda_{max}\\) (a homogeneous process).\nAccept the candidate at time \\(t_i\\) with probability \\(P = \\lambda(t_i) / \\lambda_{max}\\).\nIf rejected, discard the event and generate the next candidate from the current time.\n\nThis essentially generates a dense cloud of points (at the high rate) and thins them out where the actual intensity \\(\\lambda(t)\\) is low.\n\n\n\n\n#' Simulate Inhomogeneous Poisson Process via Thinning\n#'\n#' @param intensity_fn A function that takes time 't' and returns lambda(t)\n#' @param max_rate A constant upper bound for intensity_fn over the duration\n#' @param duration Total simulation time\n#' @return Vector of event times\nsim_inhomogeneous_thinning &lt;- function(intensity_fn, max_rate, duration) {\n  event_times &lt;- numeric(0)\n  current_time &lt;- 0\n  \n  while(current_time &lt; duration) {\n    # 1. Generate time to next candidate event (Homogeneous step)\n    # Using rate = max_rate\n    dt &lt;- rexp(1, rate = max_rate)\n    current_time &lt;- current_time + dt\n    \n    if (current_time &gt; duration) break\n    \n    # 2. Rejection Step\n    # Calculate actual intensity at this candidate time\n    lambda_actual &lt;- intensity_fn(current_time)\n    \n    # Calculate acceptance probability\n    prob_accept &lt;- lambda_actual / max_rate\n    \n    # Check bounds (rigor check)\n    if (prob_accept &gt; 1) {\n      stop(\"Error: max_rate provided is lower than actual intensity.\")\n    }\n    \n    # 3. Accept or Reject\n    # runif(1) generates a random number between 0 and 1\n    if (runif(1) &lt; prob_accept) {\n      event_times &lt;- c(event_times, current_time)\n    }\n  }\n  \n  return(event_times)\n}\n\n# Define a time-varying intensity function (e.g., Sine wave stimulus)\n# Baseline 10Hz, Modulation +/- 5Hz, Frequency 1Hz\nsine_intensity &lt;- function(t) {\n  10 + 5 * sin(2 * pi * t)\n}\n\n# The maximum rate is 10 + 5 = 15 Hz\nlambda_max &lt;- 15\nT_max &lt;- 5\n\nset.seed(123)\nspikes_inhomo &lt;- sim_inhomogeneous_thinning(sine_intensity, lambda_max, T_max)\n\n# Visualization\npar(mfrow = c(2, 1), mar = c(4, 4, 2, 2))\n# Plot Intensity Function\ncurve(sine_intensity(x), from = 0, to = T_max, ylab = \"Intensity (Hz)\", \n      main = \"Inhomogeneous Intensity and Spikes\", ylim = c(0, 16))\n# Overlay spikes\npoints(spikes_inhomo, rep(0.5, length(spikes_inhomo)), pch = \"|\", col = \"blue\")\n\n# Check spike density vs time\nhist(spikes_inhomo, breaks = 20, main = \"Spike Count Histogram\", \n     xlab = \"Time (s)\", border = \"white\", col = \"grey\")\n\n\n\n\n\n\n\n\nThe spike density in the histogram tracks the sine wave of the intensity function.\n\n\n\n\nScenario: A real neuron cannot fire twice in immediate succession.\nAfter a spike, sodium channels are inactivated (absolute refractory period), followed by a recovery period (relative refractory period).\nThe intensity is now conditional on history: \\(\\lambda(t | H_t)\\). specifically, it depends on the time elapsed since the last spike, denoted as \\(t - t_{last}\\).\n\\[\n\\lambda(t | H_t) = \\lambda_0 \\cdot R(t - t_{last})\n\\]\nWhere \\(\\lambda_0\\) is a baseline rate and \\(R(\\tau)\\) is a recovery function that goes from 0 to 1.\n\n\nWe can still use Ogata’s thinning, but we must update the intensity calculation at every step to look at the most recent accepted spike. Since the recovery function usually lowers the rate, the baseline \\(\\lambda_0\\) serves as a valid global upper bound \\(\\lambda_{max}\\).\n\n\n\n\n#' Simulate Process with Refractory Period\n#'\n#' @param baseline_rate The firing rate when fully recovered\n#' @param abs_refractory Absolute refractory period duration (seconds)\n#' @param tau_recovery Time constant for relative refractory recovery\n#' @param duration Total simulation time\nsim_refractory &lt;- function(baseline_rate, abs_refractory, tau_recovery, duration) {\n  event_times &lt;- numeric(0)\n  current_time &lt;- 0\n  last_spike_time &lt;- -Inf # Assume long time since last spike at start\n  \n  # Upper bound is the baseline rate (recovery function &lt;= 1)\n  max_rate &lt;- baseline_rate\n  \n  while(current_time &lt; duration) {\n    # Generate candidate\n    dt &lt;- rexp(1, rate = max_rate)\n    current_time &lt;- current_time + dt\n    \n    if (current_time &gt; duration) break\n    \n    # Calculate recovery factor R(t - t_last)\n    time_since_last &lt;- current_time - last_spike_time\n    \n    if (time_since_last &lt;= abs_refractory) {\n      recovery_factor &lt;- 0 # Absolute refractory\n    } else {\n      # Exponential recovery 1 - exp(-(t - t_abs)/tau)\n      rel_time &lt;- time_since_last - abs_refractory\n      recovery_factor &lt;- 1 - exp(-rel_time / tau_recovery)\n    }\n    \n    lambda_actual &lt;- baseline_rate * recovery_factor\n    \n    # Accept/Reject\n    if (runif(1) &lt; (lambda_actual / max_rate)) {\n      event_times &lt;- c(event_times, current_time)\n      last_spike_time &lt;- current_time # Update history\n    }\n  }\n  return(event_times)\n}\n\n# Parameters\nbase_rate &lt;- 20 # Hz\nt_abs &lt;- 0.010  # 10ms absolute refractory\ntau &lt;- 0.020    # 20ms recovery time constant\nT_sim &lt;- 10\n\nset.seed(999)\nspikes_ref &lt;- sim_refractory(base_rate, t_abs, tau, T_sim)\n\n# Visualization\n# ISI Histogram is key here. It should show 0 counts near 0.\nhist(diff(spikes_ref), breaks = seq(0, 0.5, by=0.01), \n     main = \"ISI Histogram (Refractory)\", \n     xlab = \"ISI (s)\", col = \"lightblue\", border = \"white\")\nabline(v = t_abs, col = \"red\", lty = 2, lwd = 2)\ntext(t_abs, 5, \"Abs. Refractory\", pos = 4, col = \"red\")\n\n\n\n\n\n\n\n\nThe histogram shows a “hole” at small intervals (0 to 10ms), characterizing the refractory period. A standard Poisson process would have its highest density at zero.\n\n\n\n\nScenario: “Bursting” behavior.\nWhen a neuron fires (or a vesicle is released), it temporarily increases the probability of another event occurring immediately after. This mimics network excitation or calcium influx.\nThe intensity is defined as a baseline plus a summation of kernels over all past events:\n\\[\n\\lambda(t | H_t) = \\mu + \\sum_{t_i &lt; t} \\alpha e^{-\\beta (t - t_i)}\n\\]\n\n\\(\\mu\\): Baseline background rate.\n\\(\\alpha\\): Magnitude of excitation (jump size).\n\\(\\beta\\): Decay rate of the excitation.\n\n\n\nThis is more complex because every time an event occurs, the intensity jumps up. The constant upper bound strategy becomes inefficient or impossible if the self-excitation is strong.\nWe use a dynamic upper bound. Between events, the intensity is strictly decaying (since the exponential kernels decay). Therefore, immediately after an event at \\(t_i\\), the intensity is at a local maximum. We can use the current intensity value as the bound for the next interval, or update the bound adaptively.\nHere, we define a function get_hawkes_intensity that sums the contributions of all previous spikes.\n\n\n\n\n#' Simulate Hawkes Process (Self-Exciting)\n#'\n#' @param mu Baseline rate\n#' @param alpha Excitation magnitude\n#' @param beta Decay rate\n#' @param duration Simulation time\nsim_hawkes &lt;- function(mu, alpha, beta, duration) {\n  event_times &lt;- numeric(0)\n  current_time &lt;- 0\n  \n  # We need a dynamic upper bound.\n  # At t=0, max lambda is just mu.\n  current_max_lambda &lt;- mu \n  \n  while(current_time &lt; duration) {\n    # 1. Generate candidate based on current conservative upper bound\n    # Note: If the process is very bursty, lambda can get high.\n    \n    # We construct the upper bound at the CURRENT time.\n    # Since the Hawkes kernel decays, the intensity at current_time\n    # is the highest it will be until the next event.\n    \n    # Calculate intensity at exactly current_time\n    if (length(event_times) == 0) {\n      lambda_now &lt;- mu\n    } else {\n      # Sum of alpha * exp(-beta * (t - t_i))\n      decay_factors &lt;- exp(-beta * (current_time - event_times))\n      lambda_now &lt;- mu + sum(alpha * decay_factors)\n    }\n    \n    # Use lambda_now as the bounding lambda for the next step\n    # because the function is monotonically decreasing between spikes.\n    lambda_bound &lt;- lambda_now\n    \n    # Generate candidate step\n    dt &lt;- rexp(1, rate = lambda_bound)\n    candidate_time &lt;- current_time + dt\n    \n    if (candidate_time &gt; duration) break\n    \n    # 2. Rejection Test\n    # Calculate actual intensity at candidate_time\n    if (length(event_times) == 0) {\n      lambda_candidate &lt;- mu\n    } else {\n      decay_factors &lt;- exp(-beta * (candidate_time - event_times))\n      lambda_candidate &lt;- mu + sum(alpha * decay_factors)\n    }\n    \n    prob_accept &lt;- lambda_candidate / lambda_bound\n    \n    if (runif(1) &lt; prob_accept) {\n      event_times &lt;- c(event_times, candidate_time)\n      # Do not update current_time here; the loop continues from candidate_time\n    }\n    \n    # Advance time regardless of acceptance\n    current_time &lt;- candidate_time\n  }\n  \n  return(event_times)\n}\n\n# Parameters: rare background events, but strong bursting\nmu_bg &lt;- 2      # 2 Hz spontaneous\nalpha_ex &lt;- 10  # Jump by 10 Hz after a spike\nbeta_dec &lt;- 20  # Decay back quickly (within ~100ms)\nT_sim &lt;- 5\n\nset.seed(2026)\nspikes_hawkes &lt;- sim_hawkes(mu_bg, alpha_ex, beta_dec, T_sim)\n\n# Visualization\npar(mfrow = c(1, 1))\nplot(spikes_hawkes, rep(1, length(spikes_hawkes)), type = \"n\", \n     ylim = c(0, 2), xlim = c(0, T_sim), xlab = \"Time (s)\", yaxt = \"n\", ylab = \"\")\nsegments(spikes_hawkes, 0.8, spikes_hawkes, 1.2, lwd = 1)\ntitle(\"Hawkes Process (Bursting)\")\ntext(0, 1.5, paste(\"Total Spikes:\", length(spikes_hawkes)), pos=4)\n\n\n\n\n\n\n\n\nIn the resulting plot, you will observe distinct clusters of events. A spontaneous background event occurs, triggering a rapid succession of subsequent events that eventually die out as the decay (\\(\\beta\\)) overcomes the excitation (\\(\\alpha\\)).\n\n\n\n\nRejection sampling (thinning) is intuitive but can be computationally slow if \\(\\lambda_{max}\\) is much larger than the average \\(\\lambda(t)\\) (high rejection rate). The Time-Rescaling Theorem offers a direct generation method.\nTheory: For any point process with conditional intensity \\(\\lambda(t | H_t)\\), if we transform the time axis using the integrated intensity:\n\\[\n\\Lambda(t) = \\int_0^t \\lambda(u | H_u) du\n\\]\nThe transformed event times \\(\\tau_i = \\Lambda(t_i)\\) form a Homogeneous Poisson Process with rate 1.\nSimulation Algorithm:\n\nGenerate unit-rate exponential intervals \\(E_1, E_2, \\dots\\) (which sum to \\(\\tau_1, \\tau_2, \\dots\\)).\nFind the actual time \\(t_i\\) such that \\(\\int_{t_{i-1}}^{t_i} \\lambda(u) du = E_i\\).\n\nThis requires inverting the integral of the intensity function. For simple functions, this is analytical. For complex functions, we solve it numerically.\nBelow is an implementation for a time-dependent sine wave (same as Section 2) using numerical root finding instead of thinning.\n\n\n\n#' Simulate via Time-Rescaling (Inversion) - Corrected\n#'\n#' @param integrated_intensity_fn Function Lambda(t) (The definite integral of lambda from 0 to t)\n#' @param duration Total time to simulate (seconds)\n#' @return Vector of event times\nsim_inversion &lt;- function(integrated_intensity_fn, duration) {\n  t_last &lt;- 0\n  event_times &lt;- numeric(0)\n  \n  while(TRUE) {\n    # 1. Generate the \"target area\" (integral step) required to trigger next event\n    # For a Poisson process, these steps are Exponential(1)\n    target_integral &lt;- rexp(1, rate = 1)\n    \n    # 2. Calculate current accumulated intensity\n    Lambda_current &lt;- integrated_intensity_fn(t_last)\n    \n    # 3. Determine target accumulated intensity\n    Lambda_target &lt;- Lambda_current + target_integral\n    \n    # 4. Safety check: Is the target reachable within the simulation duration?\n    # If the total area under the curve up to 'duration' is less than target, stop.\n    if (integrated_intensity_fn(duration) &lt; Lambda_target) {\n      break \n    }\n    \n    # 5. Solve for t such that Lambda(t) = Lambda_target\n    f_root &lt;- function(t) { integrated_intensity_fn(t) - Lambda_target }\n    \n    # Use uniroot to find the time. extendInt=\"upX\" allows searching \n    # beyond the initial guess if the rate is unexpectedly low.\n    solution &lt;- uniroot(f_root, lower = t_last, upper = duration * 2, extendInt = \"upX\")\n    t_next &lt;- solution$root\n    \n    if (t_next &gt; duration) break\n    \n    event_times &lt;- c(event_times, t_next)\n    t_last &lt;- t_next\n  }\n  return(event_times)\n}\n\n# --- Example Usage ---\n\n# Define Integral of Sine Intensity: 10 + 5sin(2*pi*t)\n# Integral = 10t - (5/2pi)cos(2*pi*t) + C\n# We define C such that Lambda(0) = 0.\nLambda_sine &lt;- function(t) {\n  10 * t - (5 / (2 * pi)) * cos(2 * pi * t) + (5 / (2 * pi))\n}\n\nset.seed(123)\nspikes_inv &lt;- sim_inversion(Lambda_sine, duration = 5)\n\n# Verification\ncat(\"Number of events simulated:\", length(spikes_inv), \"\\n\")\n\nNumber of events simulated: 45 \n\n\nThe inversion method is precise and generates no rejected points, making it highly efficient for smooth, integrable intensity functions.\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nComplexity\nBest For\nPhysiology Use Case\n\n\n\n\nHomogeneous\nO(N)\nConstant rates\nBackground noise, spontaneous firing\n\n\nThinning\nO(N / acceptance_ratio)\nComplex/Non-integrable functions\nArbitrary stimulus waveforms\n\n\nModified Thinning\nHigh\nHistory dependence\nRefractory periods, bursting, network models\n\n\nTime-Rescaling\nO(N * root_finding)\nIntegrable functions\nSmoothly varying stimuli, oscillating inputs\n\n\n\n\n\n\nWhen building simulations for publication or analysis:\n\nVectorize where possible: The homogeneous simulation contains no loops. It is orders of magnitude faster than iterative approaches.\nModularize Intensity: Define your physiological model (refractory, bursting, stimulus) as a separate S3 object or function, then pass it to a generic simulation engine (like the thinning algorithm).\nValidate: Always plot the ISI histogram and the autocorrelogram of your simulated data. If you simulate a refractory period, the autocorrelogram must be zero at lag zero.\n\nThis guide provides the algorithmic primitives. You can combine them—for example, a Hawkes process with a time-varying baseline \\(\\mu(t)\\)—simply by updating the intensity function \\(\\lambda(t)\\) inside the thinning loop."
  },
  {
    "objectID": "docs/point-process.html#mathematical-foundations",
    "href": "docs/point-process.html#mathematical-foundations",
    "title": "Simulating Point Processes in R",
    "section": "",
    "text": "The conditional intensity function is defined as:\n\\[\n\\lambda(t | H_t) = \\lim_{\\Delta t \\to 0} \\frac{P(\\text{event in } (t, t+\\Delta t] | H_t)}{\\Delta t}\n\\]\nIf \\(\\lambda(t | H_t)\\) is constant, the process is a Homogeneous Poisson Process. If \\(\\lambda(t | H_t)\\) depends on time \\(t\\) but not history \\(H_t\\), it is an Inhomogeneous Poisson Process. If it depends on \\(H_t\\), it is a general point process (e.g., a renewal process or Hawkes process)."
  },
  {
    "objectID": "docs/point-process.html#the-homogeneous-poisson-process",
    "href": "docs/point-process.html#the-homogeneous-poisson-process",
    "title": "Simulating Point Processes in R",
    "section": "",
    "text": "Scenario: A neuron firing due to random thermal noise with no external stimulus and no refractory period.\nThis is the simplest baseline. The rate \\(\\lambda\\) is constant. A defining property of the homogeneous Poisson process is that the time intervals between events (Inter-Spike Intervals, or ISIs) follow an Exponential distribution.\nTo simulate this, we do not need complex loops. We generate the intervals directly and take their cumulative sum to find the event times.\n\n\n\n#' Simulate Homogeneous Poisson Process\n#' \n#' @param rate The constant firing rate (Hz)\n#' @param duration Total time to simulate (seconds)\n#' @return Vector of event times\nsim_homogeneous &lt;- function(rate, duration) {\n  # 1. Estimate expected number of events to allocate memory\n  # We add a margin (e.g., 20%) to ensure we cover the duration\n  n_guess &lt;- ceiling(rate * duration * 1.2)\n  \n  # 2. Generate Inter-Spike Intervals (ISIs) from Exponential distribution\n  # The rate parameter for rexp is lambda\n  isis &lt;- rexp(n_guess, rate = rate)\n  \n  # 3. Convert intervals to absolute times\n  event_times &lt;- cumsum(isis)\n  \n  # 4. Filter events that exceed the duration\n  event_times &lt;- event_times[event_times &lt;= duration]\n  \n  return(event_times)\n}\n\n# Example Usage\nset.seed(42)\nlambda_constant &lt;- 10 # 10 Hz\nT_max &lt;- 10           # 10 seconds\n\nspikes_homo &lt;- sim_homogeneous(lambda_constant, T_max)\n\n# Visualization\npar(mfrow = c(1, 2))\n# Raster plot (first 1 second)\nstripchart(spikes_homo[spikes_homo &lt; 1], method = \"jitter\", pch = \"|\", \n           main = \"Raster Plot (First 1s)\", xlab = \"Time (s)\")\n# Histogram of ISIs\nhist(diff(spikes_homo), breaks = 30, main = \"ISI Histogram\", \n     xlab = \"Inter-Spike Interval (s)\", freq = FALSE)\ncurve(dexp(x, rate = lambda_constant), add = TRUE, col = \"red\")\n\n\n\n\n\n\n\n\nThe histogram of ISIs aligns with the theoretical exponential density curve (red line), confirming the memoryless nature of the process."
  },
  {
    "objectID": "docs/point-process.html#the-inhomogeneous-poisson-process",
    "href": "docs/point-process.html#the-inhomogeneous-poisson-process",
    "title": "Simulating Point Processes in R",
    "section": "",
    "text": "Scenario: A retinal neuron responding to a visual stimulus that changes brightness over time. The firing rate changes, but the neuron still has no “memory” (no refractory period).\nHere, \\(\\lambda(t)\\) varies with time. We can no longer simply draw from an exponential distribution because the rate parameter changes continuously.\n\n\nThe standard method for simulating this is Ogata’s Thinning Algorithm (a variation of Lewis and Shedler’s method). The logic follows a “generate and reject” principle:\n\nDetermine an upper bound \\(\\lambda_{max}\\) such that \\(\\lambda(t) \\le \\lambda_{max}\\) for all \\(t\\).\nGenerate a candidate event time using the constant rate \\(\\lambda_{max}\\) (a homogeneous process).\nAccept the candidate at time \\(t_i\\) with probability \\(P = \\lambda(t_i) / \\lambda_{max}\\).\nIf rejected, discard the event and generate the next candidate from the current time.\n\nThis essentially generates a dense cloud of points (at the high rate) and thins them out where the actual intensity \\(\\lambda(t)\\) is low.\n\n\n\n\n#' Simulate Inhomogeneous Poisson Process via Thinning\n#'\n#' @param intensity_fn A function that takes time 't' and returns lambda(t)\n#' @param max_rate A constant upper bound for intensity_fn over the duration\n#' @param duration Total simulation time\n#' @return Vector of event times\nsim_inhomogeneous_thinning &lt;- function(intensity_fn, max_rate, duration) {\n  event_times &lt;- numeric(0)\n  current_time &lt;- 0\n  \n  while(current_time &lt; duration) {\n    # 1. Generate time to next candidate event (Homogeneous step)\n    # Using rate = max_rate\n    dt &lt;- rexp(1, rate = max_rate)\n    current_time &lt;- current_time + dt\n    \n    if (current_time &gt; duration) break\n    \n    # 2. Rejection Step\n    # Calculate actual intensity at this candidate time\n    lambda_actual &lt;- intensity_fn(current_time)\n    \n    # Calculate acceptance probability\n    prob_accept &lt;- lambda_actual / max_rate\n    \n    # Check bounds (rigor check)\n    if (prob_accept &gt; 1) {\n      stop(\"Error: max_rate provided is lower than actual intensity.\")\n    }\n    \n    # 3. Accept or Reject\n    # runif(1) generates a random number between 0 and 1\n    if (runif(1) &lt; prob_accept) {\n      event_times &lt;- c(event_times, current_time)\n    }\n  }\n  \n  return(event_times)\n}\n\n# Define a time-varying intensity function (e.g., Sine wave stimulus)\n# Baseline 10Hz, Modulation +/- 5Hz, Frequency 1Hz\nsine_intensity &lt;- function(t) {\n  10 + 5 * sin(2 * pi * t)\n}\n\n# The maximum rate is 10 + 5 = 15 Hz\nlambda_max &lt;- 15\nT_max &lt;- 5\n\nset.seed(123)\nspikes_inhomo &lt;- sim_inhomogeneous_thinning(sine_intensity, lambda_max, T_max)\n\n# Visualization\npar(mfrow = c(2, 1), mar = c(4, 4, 2, 2))\n# Plot Intensity Function\ncurve(sine_intensity(x), from = 0, to = T_max, ylab = \"Intensity (Hz)\", \n      main = \"Inhomogeneous Intensity and Spikes\", ylim = c(0, 16))\n# Overlay spikes\npoints(spikes_inhomo, rep(0.5, length(spikes_inhomo)), pch = \"|\", col = \"blue\")\n\n# Check spike density vs time\nhist(spikes_inhomo, breaks = 20, main = \"Spike Count Histogram\", \n     xlab = \"Time (s)\", border = \"white\", col = \"grey\")\n\n\n\n\n\n\n\n\nThe spike density in the histogram tracks the sine wave of the intensity function."
  },
  {
    "objectID": "docs/point-process.html#history-dependence-the-refractory-period",
    "href": "docs/point-process.html#history-dependence-the-refractory-period",
    "title": "Simulating Point Processes in R",
    "section": "",
    "text": "Scenario: A real neuron cannot fire twice in immediate succession.\nAfter a spike, sodium channels are inactivated (absolute refractory period), followed by a recovery period (relative refractory period).\nThe intensity is now conditional on history: \\(\\lambda(t | H_t)\\). specifically, it depends on the time elapsed since the last spike, denoted as \\(t - t_{last}\\).\n\\[\n\\lambda(t | H_t) = \\lambda_0 \\cdot R(t - t_{last})\n\\]\nWhere \\(\\lambda_0\\) is a baseline rate and \\(R(\\tau)\\) is a recovery function that goes from 0 to 1.\n\n\nWe can still use Ogata’s thinning, but we must update the intensity calculation at every step to look at the most recent accepted spike. Since the recovery function usually lowers the rate, the baseline \\(\\lambda_0\\) serves as a valid global upper bound \\(\\lambda_{max}\\).\n\n\n\n\n#' Simulate Process with Refractory Period\n#'\n#' @param baseline_rate The firing rate when fully recovered\n#' @param abs_refractory Absolute refractory period duration (seconds)\n#' @param tau_recovery Time constant for relative refractory recovery\n#' @param duration Total simulation time\nsim_refractory &lt;- function(baseline_rate, abs_refractory, tau_recovery, duration) {\n  event_times &lt;- numeric(0)\n  current_time &lt;- 0\n  last_spike_time &lt;- -Inf # Assume long time since last spike at start\n  \n  # Upper bound is the baseline rate (recovery function &lt;= 1)\n  max_rate &lt;- baseline_rate\n  \n  while(current_time &lt; duration) {\n    # Generate candidate\n    dt &lt;- rexp(1, rate = max_rate)\n    current_time &lt;- current_time + dt\n    \n    if (current_time &gt; duration) break\n    \n    # Calculate recovery factor R(t - t_last)\n    time_since_last &lt;- current_time - last_spike_time\n    \n    if (time_since_last &lt;= abs_refractory) {\n      recovery_factor &lt;- 0 # Absolute refractory\n    } else {\n      # Exponential recovery 1 - exp(-(t - t_abs)/tau)\n      rel_time &lt;- time_since_last - abs_refractory\n      recovery_factor &lt;- 1 - exp(-rel_time / tau_recovery)\n    }\n    \n    lambda_actual &lt;- baseline_rate * recovery_factor\n    \n    # Accept/Reject\n    if (runif(1) &lt; (lambda_actual / max_rate)) {\n      event_times &lt;- c(event_times, current_time)\n      last_spike_time &lt;- current_time # Update history\n    }\n  }\n  return(event_times)\n}\n\n# Parameters\nbase_rate &lt;- 20 # Hz\nt_abs &lt;- 0.010  # 10ms absolute refractory\ntau &lt;- 0.020    # 20ms recovery time constant\nT_sim &lt;- 10\n\nset.seed(999)\nspikes_ref &lt;- sim_refractory(base_rate, t_abs, tau, T_sim)\n\n# Visualization\n# ISI Histogram is key here. It should show 0 counts near 0.\nhist(diff(spikes_ref), breaks = seq(0, 0.5, by=0.01), \n     main = \"ISI Histogram (Refractory)\", \n     xlab = \"ISI (s)\", col = \"lightblue\", border = \"white\")\nabline(v = t_abs, col = \"red\", lty = 2, lwd = 2)\ntext(t_abs, 5, \"Abs. Refractory\", pos = 4, col = \"red\")\n\n\n\n\n\n\n\n\nThe histogram shows a “hole” at small intervals (0 to 10ms), characterizing the refractory period. A standard Poisson process would have its highest density at zero."
  },
  {
    "objectID": "docs/point-process.html#self-exciting-processes-hawkes-process",
    "href": "docs/point-process.html#self-exciting-processes-hawkes-process",
    "title": "Simulating Point Processes in R",
    "section": "",
    "text": "Scenario: “Bursting” behavior.\nWhen a neuron fires (or a vesicle is released), it temporarily increases the probability of another event occurring immediately after. This mimics network excitation or calcium influx.\nThe intensity is defined as a baseline plus a summation of kernels over all past events:\n\\[\n\\lambda(t | H_t) = \\mu + \\sum_{t_i &lt; t} \\alpha e^{-\\beta (t - t_i)}\n\\]\n\n\\(\\mu\\): Baseline background rate.\n\\(\\alpha\\): Magnitude of excitation (jump size).\n\\(\\beta\\): Decay rate of the excitation.\n\n\n\nThis is more complex because every time an event occurs, the intensity jumps up. The constant upper bound strategy becomes inefficient or impossible if the self-excitation is strong.\nWe use a dynamic upper bound. Between events, the intensity is strictly decaying (since the exponential kernels decay). Therefore, immediately after an event at \\(t_i\\), the intensity is at a local maximum. We can use the current intensity value as the bound for the next interval, or update the bound adaptively.\nHere, we define a function get_hawkes_intensity that sums the contributions of all previous spikes.\n\n\n\n\n#' Simulate Hawkes Process (Self-Exciting)\n#'\n#' @param mu Baseline rate\n#' @param alpha Excitation magnitude\n#' @param beta Decay rate\n#' @param duration Simulation time\nsim_hawkes &lt;- function(mu, alpha, beta, duration) {\n  event_times &lt;- numeric(0)\n  current_time &lt;- 0\n  \n  # We need a dynamic upper bound.\n  # At t=0, max lambda is just mu.\n  current_max_lambda &lt;- mu \n  \n  while(current_time &lt; duration) {\n    # 1. Generate candidate based on current conservative upper bound\n    # Note: If the process is very bursty, lambda can get high.\n    \n    # We construct the upper bound at the CURRENT time.\n    # Since the Hawkes kernel decays, the intensity at current_time\n    # is the highest it will be until the next event.\n    \n    # Calculate intensity at exactly current_time\n    if (length(event_times) == 0) {\n      lambda_now &lt;- mu\n    } else {\n      # Sum of alpha * exp(-beta * (t - t_i))\n      decay_factors &lt;- exp(-beta * (current_time - event_times))\n      lambda_now &lt;- mu + sum(alpha * decay_factors)\n    }\n    \n    # Use lambda_now as the bounding lambda for the next step\n    # because the function is monotonically decreasing between spikes.\n    lambda_bound &lt;- lambda_now\n    \n    # Generate candidate step\n    dt &lt;- rexp(1, rate = lambda_bound)\n    candidate_time &lt;- current_time + dt\n    \n    if (candidate_time &gt; duration) break\n    \n    # 2. Rejection Test\n    # Calculate actual intensity at candidate_time\n    if (length(event_times) == 0) {\n      lambda_candidate &lt;- mu\n    } else {\n      decay_factors &lt;- exp(-beta * (candidate_time - event_times))\n      lambda_candidate &lt;- mu + sum(alpha * decay_factors)\n    }\n    \n    prob_accept &lt;- lambda_candidate / lambda_bound\n    \n    if (runif(1) &lt; prob_accept) {\n      event_times &lt;- c(event_times, candidate_time)\n      # Do not update current_time here; the loop continues from candidate_time\n    }\n    \n    # Advance time regardless of acceptance\n    current_time &lt;- candidate_time\n  }\n  \n  return(event_times)\n}\n\n# Parameters: rare background events, but strong bursting\nmu_bg &lt;- 2      # 2 Hz spontaneous\nalpha_ex &lt;- 10  # Jump by 10 Hz after a spike\nbeta_dec &lt;- 20  # Decay back quickly (within ~100ms)\nT_sim &lt;- 5\n\nset.seed(2026)\nspikes_hawkes &lt;- sim_hawkes(mu_bg, alpha_ex, beta_dec, T_sim)\n\n# Visualization\npar(mfrow = c(1, 1))\nplot(spikes_hawkes, rep(1, length(spikes_hawkes)), type = \"n\", \n     ylim = c(0, 2), xlim = c(0, T_sim), xlab = \"Time (s)\", yaxt = \"n\", ylab = \"\")\nsegments(spikes_hawkes, 0.8, spikes_hawkes, 1.2, lwd = 1)\ntitle(\"Hawkes Process (Bursting)\")\ntext(0, 1.5, paste(\"Total Spikes:\", length(spikes_hawkes)), pos=4)\n\n\n\n\n\n\n\n\nIn the resulting plot, you will observe distinct clusters of events. A spontaneous background event occurs, triggering a rapid succession of subsequent events that eventually die out as the decay (\\(\\beta\\)) overcomes the excitation (\\(\\alpha\\))."
  },
  {
    "objectID": "docs/point-process.html#the-general-inversion-method-time-rescaling",
    "href": "docs/point-process.html#the-general-inversion-method-time-rescaling",
    "title": "Simulating Point Processes in R",
    "section": "",
    "text": "Rejection sampling (thinning) is intuitive but can be computationally slow if \\(\\lambda_{max}\\) is much larger than the average \\(\\lambda(t)\\) (high rejection rate). The Time-Rescaling Theorem offers a direct generation method.\nTheory: For any point process with conditional intensity \\(\\lambda(t | H_t)\\), if we transform the time axis using the integrated intensity:\n\\[\n\\Lambda(t) = \\int_0^t \\lambda(u | H_u) du\n\\]\nThe transformed event times \\(\\tau_i = \\Lambda(t_i)\\) form a Homogeneous Poisson Process with rate 1.\nSimulation Algorithm:\n\nGenerate unit-rate exponential intervals \\(E_1, E_2, \\dots\\) (which sum to \\(\\tau_1, \\tau_2, \\dots\\)).\nFind the actual time \\(t_i\\) such that \\(\\int_{t_{i-1}}^{t_i} \\lambda(u) du = E_i\\).\n\nThis requires inverting the integral of the intensity function. For simple functions, this is analytical. For complex functions, we solve it numerically.\nBelow is an implementation for a time-dependent sine wave (same as Section 2) using numerical root finding instead of thinning.\n\n\n\n#' Simulate via Time-Rescaling (Inversion) - Corrected\n#'\n#' @param integrated_intensity_fn Function Lambda(t) (The definite integral of lambda from 0 to t)\n#' @param duration Total time to simulate (seconds)\n#' @return Vector of event times\nsim_inversion &lt;- function(integrated_intensity_fn, duration) {\n  t_last &lt;- 0\n  event_times &lt;- numeric(0)\n  \n  while(TRUE) {\n    # 1. Generate the \"target area\" (integral step) required to trigger next event\n    # For a Poisson process, these steps are Exponential(1)\n    target_integral &lt;- rexp(1, rate = 1)\n    \n    # 2. Calculate current accumulated intensity\n    Lambda_current &lt;- integrated_intensity_fn(t_last)\n    \n    # 3. Determine target accumulated intensity\n    Lambda_target &lt;- Lambda_current + target_integral\n    \n    # 4. Safety check: Is the target reachable within the simulation duration?\n    # If the total area under the curve up to 'duration' is less than target, stop.\n    if (integrated_intensity_fn(duration) &lt; Lambda_target) {\n      break \n    }\n    \n    # 5. Solve for t such that Lambda(t) = Lambda_target\n    f_root &lt;- function(t) { integrated_intensity_fn(t) - Lambda_target }\n    \n    # Use uniroot to find the time. extendInt=\"upX\" allows searching \n    # beyond the initial guess if the rate is unexpectedly low.\n    solution &lt;- uniroot(f_root, lower = t_last, upper = duration * 2, extendInt = \"upX\")\n    t_next &lt;- solution$root\n    \n    if (t_next &gt; duration) break\n    \n    event_times &lt;- c(event_times, t_next)\n    t_last &lt;- t_next\n  }\n  return(event_times)\n}\n\n# --- Example Usage ---\n\n# Define Integral of Sine Intensity: 10 + 5sin(2*pi*t)\n# Integral = 10t - (5/2pi)cos(2*pi*t) + C\n# We define C such that Lambda(0) = 0.\nLambda_sine &lt;- function(t) {\n  10 * t - (5 / (2 * pi)) * cos(2 * pi * t) + (5 / (2 * pi))\n}\n\nset.seed(123)\nspikes_inv &lt;- sim_inversion(Lambda_sine, duration = 5)\n\n# Verification\ncat(\"Number of events simulated:\", length(spikes_inv), \"\\n\")\n\nNumber of events simulated: 45 \n\n\nThe inversion method is precise and generates no rejected points, making it highly efficient for smooth, integrable intensity functions."
  },
  {
    "objectID": "docs/point-process.html#summary-of-methods",
    "href": "docs/point-process.html#summary-of-methods",
    "title": "Simulating Point Processes in R",
    "section": "",
    "text": "Method\nComplexity\nBest For\nPhysiology Use Case\n\n\n\n\nHomogeneous\nO(N)\nConstant rates\nBackground noise, spontaneous firing\n\n\nThinning\nO(N / acceptance_ratio)\nComplex/Non-integrable functions\nArbitrary stimulus waveforms\n\n\nModified Thinning\nHigh\nHistory dependence\nRefractory periods, bursting, network models\n\n\nTime-Rescaling\nO(N * root_finding)\nIntegrable functions\nSmoothly varying stimuli, oscillating inputs"
  },
  {
    "objectID": "docs/point-process.html#code-structure-for-researchers",
    "href": "docs/point-process.html#code-structure-for-researchers",
    "title": "Simulating Point Processes in R",
    "section": "",
    "text": "When building simulations for publication or analysis:\n\nVectorize where possible: The homogeneous simulation contains no loops. It is orders of magnitude faster than iterative approaches.\nModularize Intensity: Define your physiological model (refractory, bursting, stimulus) as a separate S3 object or function, then pass it to a generic simulation engine (like the thinning algorithm).\nValidate: Always plot the ISI histogram and the autocorrelogram of your simulated data. If you simulate a refractory period, the autocorrelogram must be zero at lag zero.\n\nThis guide provides the algorithmic primitives. You can combine them—for example, a Hawkes process with a time-varying baseline \\(\\mu(t)\\)—simply by updating the intensity function \\(\\lambda(t)\\) inside the thinning loop."
  },
  {
    "objectID": "docs/point-process.html#level-1-the-renewal-model-the-noisy-metronome",
    "href": "docs/point-process.html#level-1-the-renewal-model-the-noisy-metronome",
    "title": "Simulating Point Processes in R",
    "section": "Level 1: The Renewal Model (The Noisy Metronome)",
    "text": "Level 1: The Renewal Model (The Noisy Metronome)\nConcept: The heart fires at a regular interval (e.g., every 0.8s) with some random jitter. The intervals are independent of each other. Math: Instead of an Exponential distribution, we draw intervals from a Gamma or Inverse Gaussian distribution, which are “bell-shaped” but defined only for positive numbers.\n\n#' Simulate Heartbeats as a Renewal Process\n#' \n#' @param n_beats Number of beats to simulate\n#' @param mean_rr Mean R-R interval (seconds)\n#' @param shape Shape parameter (higher = more rhythmic/less variable)\nsim_heart_renewal &lt;- function(n_beats, mean_rr, shape) {\n  # We use Gamma distribution: Mean = shape / rate\n  # Therefore: rate = shape / Mean\n  rate_param &lt;- shape / mean_rr\n  \n  # Generate R-R intervals\n  rr_intervals &lt;- rgamma(n_beats, shape = shape, rate = rate_param)\n  \n  # Convert to absolute event times\n  beat_times &lt;- cumsum(rr_intervals)\n  \n  return(list(times = beat_times, rr = rr_intervals))\n}\n\nset.seed(42)\n# Simulate 60 beats, Mean RR = 0.8s (75 BPM), Shape = 50 (Very regular)\nheart_data &lt;- sim_heart_renewal(n_beats = 60, mean_rr = 0.8, shape = 50)\n\n# Visualization\npar(mfrow = c(1, 2))\n# 1. Tachogram: The standard way to view HRV\nplot(heart_data$rr, type = \"o\", pch = 16, col = \"darkred\",\n     main = \"Tachogram (Noisy Metronome)\", ylab = \"R-R Interval (s)\", xlab = \"Beat Number\")\n\n# 2. Histogram\nhist(heart_data$rr, breaks = 15, col = \"pink\", main = \"R-R Distribution\", xlab = \"Time (s)\")\n\n\n\n\n\n\n\n\nPhysiological Note: While the distribution looks correct (Gaussian-like), the Tachogram reveals the flaw: the variation is “white noise.” Real heart rates drift slowly up and down; they don’t jump randomly beat-to-beat."
  },
  {
    "objectID": "docs/point-process.html#level-2-respiratory-sinus-arrhythmia-rsa",
    "href": "docs/point-process.html#level-2-respiratory-sinus-arrhythmia-rsa",
    "title": "Simulating Point Processes in R",
    "section": "Level 2: Respiratory Sinus Arrhythmia (RSA)",
    "text": "Level 2: Respiratory Sinus Arrhythmia (RSA)\nConcept: Breathing modulates the heart rate. During inhalation, the vagus nerve is inhibited, and heart rate speeds up (R-R shortens). During exhalation, it slows down. Math: This is a Modulated Renewal Process. We cannot just add a sine wave to the times. We must modulate the rate at which the “internal clock” of the heart ticks.\nWe use the Time-Rescaling approach again, but reversed. We generate “internal” regular ticks (Gamma distributed), and then map them to real time using the integral of the breathing signal.\n\n#' Simulate RSA via Modulated Renewal Process\n#'\n#' @param duration Total time (seconds)\n#' @param mean_rr Baseline R-R interval\n#' @param breath_freq Breathing frequency (Hz)\n#' @param rsa_strength Strength of modulation (0 to 1)\nsim_heart_rsa &lt;- function(duration, mean_rr, breath_freq, rsa_strength) {\n  \n  # 1. Define the Rate Modulation Function lambda(t)\n  # Baseline rate is 1/mean_rr. Modulation oscillates around it.\n  baseline_rate &lt;- 1 / mean_rr\n  \n  lambda_fn &lt;- function(t) {\n    baseline_rate * (1 + rsa_strength * sin(2 * pi * breath_freq * t))\n  }\n  \n  # 2. Define the Integral Lambda(t)\n  # Integral of (1 + A*sin(w*t)) is t - (A/w)cos(w*t)\n  Lambda_fn &lt;- function(t) {\n    omega &lt;- 2 * pi * breath_freq\n    term1 &lt;- baseline_rate * t\n    term2 &lt;- (baseline_rate * rsa_strength / omega) * (1 - cos(omega * t))\n    return(term1 + term2)\n  }\n  \n  # 3. Generate \"Internal\" Ticks (The underlying rhythm)\n  # We estimate how many beats fit in the duration (+ buffer)\n  n_guess &lt;- ceiling(duration / mean_rr * 1.5)\n  \n  # Using Gamma to represent the regularity of the SA node\n  # Shape 100 implies a very tight rhythm, which is then stretched/squashed by RSA\n  internal_intervals &lt;- rgamma(n_guess, shape = 100, rate = 100) \n  internal_times &lt;- cumsum(internal_intervals)\n  \n  # 4. Invert Lambda(t) to find Real Times\n  # We find t such that Lambda(t) = internal_time\n  real_times &lt;- numeric(length(internal_times))\n  \n  for(i in seq_along(internal_times)) {\n    target &lt;- internal_times[i]\n    \n    # Check if target exceeds max capacity of duration\n    if(Lambda_fn(duration) &lt; target) {\n      real_times &lt;- real_times[1:(i-1)]\n      break\n    }\n    \n    # Root finding\n    f_root &lt;- function(t) { Lambda_fn(t) - target }\n    sol &lt;- uniroot(f_root, lower = 0, upper = duration * 2, extendInt = \"upX\")\n    real_times[i] &lt;- sol$root\n  }\n  \n  return(real_times)\n}\n\nset.seed(101)\nbeats_rsa &lt;- sim_heart_rsa(duration = 20, mean_rr = 0.8, breath_freq = 0.25, rsa_strength = 0.3)\nrr_rsa &lt;- diff(beats_rsa)\n\n# Visualization\npar(mfrow = c(2, 1), mar = c(4,4,2,2))\nplot(beats_rsa[-1], rr_rsa, type = \"o\", pch = 16, col = \"blue\",\n     main = \"RSA Simulation (Tachogram)\", ylab = \"R-R Interval (s)\", xlab = \"Time (s)\")\nabline(h = 0.8, lty = 2)\n\n# Verify the spectrum (Physiologists love PSD plots)\nspec &lt;- spectrum(rr_rsa, plot = FALSE)\nplot(spec$freq, spec$spec, type = \"l\", main = \"Power Spectrum Density\", \n     xlab = \"Frequency (Cycles/Beat)\", ylab = \"Power\", xlim = c(0, 0.5))\n\n\n\n\n\n\n\n# We should see a peak around 0.25 Hz (normalized to beat frequency)\n\nThe Tachogram now shows a clear oscillation. This mimics the “Vagal Tone” effect."
  },
  {
    "objectID": "docs/point-process.html#level-3-the-barbieri-brown-model-history-dependent",
    "href": "docs/point-process.html#level-3-the-barbieri-brown-model-history-dependent",
    "title": "Simulating Point Processes in R",
    "section": "Level 3: The Barbieri-Brown Model (History Dependent)",
    "text": "Level 3: The Barbieri-Brown Model (History Dependent)\nConcept: The gold standard in statistical physiology. The probability of the next heartbeat depends explicitly on the previous interval. This creates an autoregressive structure, mimicking the baroreflex feedback loop (blood pressure changes \\(\\rightarrow\\) heart rate correction).\nMath: We model the -th R-R interval (\\(rr_k\\)) directly using a regression equation:\n\\[\nrr_k = \\mu + \\alpha (rr_{k-1} - \\mu) + \\beta_{Resp} \\cdot Resp(t) + \\epsilon\n\\]\nwhere \\(\\epsilon\\) follows an Inverse Gaussian distribution.\nThis is technically a discrete point process simulation, which is computationally efficient and widely used for generating synthetic ECGs.\n\n#' Simulate Autoregressive Heart Rate (Barbieri-Brown style)\n#'\n#' @param n_beats Total beats\n#' @param mu Mean R-R interval\n#' @param alpha Autoregressive coefficient (0 to 1). High = strong memory.\n#' @param sigma Noise level\nsim_heart_ar &lt;- function(n_beats, mu, alpha, sigma) {\n  rr &lt;- numeric(n_beats)\n  # Initialize with the mean\n  rr[1] &lt;- mu \n  \n  for(k in 2:n_beats) {\n    # 1. Calculate the conditional mean based on history\n    # This is the \"deterministic\" part of the physiology\n    past_diff &lt;- rr[k-1] - mu\n    conditional_mean &lt;- mu + alpha * past_diff\n    \n    # 2. Add stochasticity (Inverse Gaussian noise)\n    # We use a Normal approximation here for simplicity and standard R availability,\n    # but strictly it should be InvGauss for positive skewness.\n    # To prevent negative intervals in Normal approx, we add a max() check.\n    noise &lt;- rnorm(1, mean = 0, sd = sigma)\n    \n    next_rr &lt;- conditional_mean + noise\n    rr[k] &lt;- max(0.3, next_rr) # Physiologic lower bound (300ms)\n  }\n  \n  beat_times &lt;- cumsum(rr)\n  return(list(times = beat_times, rr = rr))\n}\n\nset.seed(2025)\n# Alpha = 0.8 implies strong correlation with the previous beat (Low Frequency drift)\ndata_ar &lt;- sim_heart_ar(n_beats = 200, mu = 0.8, alpha = 0.85, sigma = 0.04)\n\n# Visualization\npar(mfrow = c(1, 2))\nplot(data_ar$rr, type = \"l\", col = \"darkgreen\", lwd = 1.5,\n     main = \"Autoregressive HRV\", ylab = \"R-R Interval (s)\", xlab = \"Beat Number\")\n\n# Poincaré Plot: The signature visualization for HRV\n# Plots RR_n vs RR_n+1. A cigar shape indicates healthy autoregression.\nrr_n &lt;- data_ar$rr[1:(length(data_ar$rr)-1)]\nrr_n1 &lt;- data_ar$rr[2:length(data_ar$rr)]\n\nplot(rr_n, rr_n1, pch = 16, col = rgb(0,0,0,0.5),\n     main = \"Poincaré Plot\", xlab = \"RR[n]\", ylab = \"RR[n+1]\")\nabline(0, 1, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nInterpretation:\n\nTachogram: Shows “wandering” baselines and low-frequency trends, unlike the white noise of Level 1.\nPoincaré Plot: The points form an elongated cloud along the diagonal. A globular/round cloud indicates randomness (fibrillation/noise); a “cigar” shape indicates healthy autonomic control."
  }
]